<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_38.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><span>[39]</span><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_40.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit c99d1e6e83b06744c75d9f5e491ed495a7086b7b
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Aug 23 17:47:28 2014 -0400

    ext4: fix BUG_ON in mb_free_blocks()
    
    If we suffer a block allocation failure (for example due to a memory
    allocation failure), it's possible that we will call
    ext4_discard_allocated_blocks() before we've actually allocated any
    blocks.  In that case, fe_len and fe_start in ac-&gt;ac_f_ex will still
    be zero, and this will result in mb_free_blocks(inode, e4b, 0, 0)
    triggering the BUG_ON on mb_free_blocks():
    
            BUG_ON(last &gt;= (sb-&gt;s_blocksize &lt;&lt; 3));
    
    Fix this by bailing out of ext4_discard_allocated_blocks() if fs_len
    is zero.
    
    Also fix a missing ext4_mb_unload_buddy() call in
    ext4_discard_allocated_blocks().
    
    Google-Bug-Id: 16844242
    
    Fixes: 86f0afd463215fc3e58020493482faa4ac3a4d69
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 956027711faf..8b0f9ef517d6 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -1412,6 +1412,8 @@ static void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
 	int last = first + count - 1;
 	struct super_block *sb = e4b-&gt;bd_sb;
 
+	if (WARN_ON(count == 0))
+		return;
 	BUG_ON(last &gt;= (sb-&gt;s_blocksize &lt;&lt; 3));
 	assert_spin_locked(ext4_group_lock_ptr(sb, e4b-&gt;bd_group));
 	/* Don't bother if the block group is corrupt. */
@@ -3221,6 +3223,8 @@ static void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)
 	int err;
 
 	if (pa == NULL) {
+		if (ac-&gt;ac_f_ex.fe_len == 0)
+			return;
 		err = ext4_mb_load_buddy(ac-&gt;ac_sb, ac-&gt;ac_f_ex.fe_group, &amp;e4b);
 		if (err) {
 			/*
@@ -3235,6 +3239,7 @@ static void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)
 		mb_free_blocks(ac-&gt;ac_inode, &amp;e4b, ac-&gt;ac_f_ex.fe_start,
 			       ac-&gt;ac_f_ex.fe_len);
 		ext4_unlock_group(ac-&gt;ac_sb, ac-&gt;ac_f_ex.fe_group);
+		ext4_mb_unload_buddy(&amp;e4b);
 		return;
 	}
 	if (pa-&gt;pa_type == MB_INODE_PA)</pre><hr><pre>commit 36de928641ee48b2078d3fe9514242aaa2f92013
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Aug 23 17:47:19 2014 -0400

    ext4: propagate errors up to ext4_find_entry()'s callers
    
    If we run into some kind of error, such as ENOMEM, while calling
    ext4_getblk() or ext4_dx_find_entry(), we need to make sure this error
    gets propagated up to ext4_find_entry() and then to its callers.  This
    way, transient errors such as ENOMEM can get propagated to the VFS.
    This is important so that the system calls return the appropriate
    error, and also so that in the case of ext4_lookup(), we return an
    error instead of a NULL inode, since that will result in a negative
    dentry cache entry that will stick around long past the OOM condition
    which caused a transient ENOMEM error.
    
    Google-Bug-Id: #17142205
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 5b19760b1de5..4d95c3301775 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1825,7 +1825,7 @@ ext4_group_first_block_no(struct super_block *sb, ext4_group_t group_no)
 /*
  * Special error return code only used by dx_probe() and its callers.
  */
-#define ERR_BAD_DX_DIR	-75000
+#define ERR_BAD_DX_DIR	(-(MAX_ERRNO - 1))
 
 /*
  * Timeout and state flag for lazy initialization inode thread.
diff --git a/fs/ext4/namei.c b/fs/ext4/namei.c
index b147a67baa0d..ae7088b446d1 100644
--- a/fs/ext4/namei.c
+++ b/fs/ext4/namei.c
@@ -1227,7 +1227,7 @@ static struct buffer_head * ext4_find_entry (struct inode *dir,
 				   buffer */
 	int num = 0;
 	ext4_lblk_t  nblocks;
-	int i, err;
+	int i, err = 0;
 	int namelen;
 
 	*res_dir = NULL;
@@ -1264,7 +1264,11 @@ static struct buffer_head * ext4_find_entry (struct inode *dir,
 		 * return.  Otherwise, fall back to doing a search the
 		 * old fashioned way.
 		 */
-		if (bh || (err != ERR_BAD_DX_DIR))
+		if (err == -ENOENT)
+			return NULL;
+		if (err &amp;&amp; err != ERR_BAD_DX_DIR)
+			return ERR_PTR(err);
+		if (bh)
 			return bh;
 		dxtrace(printk(KERN_DEBUG "ext4_find_entry: dx failed, "
 			       "falling back\n"));
@@ -1295,6 +1299,11 @@ static struct buffer_head * ext4_find_entry (struct inode *dir,
 				}
 				num++;
 				bh = ext4_getblk(NULL, dir, b++, 0, &amp;err);
+				if (unlikely(err)) {
+					if (ra_max == 0)
+						return ERR_PTR(err);
+					break;
+				}
 				bh_use[ra_max] = bh;
 				if (bh)
 					ll_rw_block(READ | REQ_META | REQ_PRIO,
@@ -1417,6 +1426,8 @@ static struct dentry *ext4_lookup(struct inode *dir, struct dentry *dentry, unsi
 		return ERR_PTR(-ENAMETOOLONG);
 
 	bh = ext4_find_entry(dir, &amp;dentry-&gt;d_name, &amp;de, NULL);
+	if (IS_ERR(bh))
+		return (struct dentry *) bh;
 	inode = NULL;
 	if (bh) {
 		__u32 ino = le32_to_cpu(de-&gt;inode);
@@ -1450,6 +1461,8 @@ struct dentry *ext4_get_parent(struct dentry *child)
 	struct buffer_head *bh;
 
 	bh = ext4_find_entry(child-&gt;d_inode, &amp;dotdot, &amp;de, NULL);
+	if (IS_ERR(bh))
+		return (struct dentry *) bh;
 	if (!bh)
 		return ERR_PTR(-ENOENT);
 	ino = le32_to_cpu(de-&gt;inode);
@@ -2727,6 +2740,8 @@ static int ext4_rmdir(struct inode *dir, struct dentry *dentry)
 
 	retval = -ENOENT;
 	bh = ext4_find_entry(dir, &amp;dentry-&gt;d_name, &amp;de, NULL);
+	if (IS_ERR(bh))
+		return PTR_ERR(bh);
 	if (!bh)
 		goto end_rmdir;
 
@@ -2794,6 +2809,8 @@ static int ext4_unlink(struct inode *dir, struct dentry *dentry)
 
 	retval = -ENOENT;
 	bh = ext4_find_entry(dir, &amp;dentry-&gt;d_name, &amp;de, NULL);
+	if (IS_ERR(bh))
+		return PTR_ERR(bh);
 	if (!bh)
 		goto end_unlink;
 
@@ -3121,6 +3138,8 @@ static int ext4_find_delete_entry(handle_t *handle, struct inode *dir,
 	struct ext4_dir_entry_2 *de;
 
 	bh = ext4_find_entry(dir, d_name, &amp;de, NULL);
+	if (IS_ERR(bh))
+		return PTR_ERR(bh);
 	if (bh) {
 		retval = ext4_delete_entry(handle, dir, de, bh);
 		brelse(bh);
@@ -3202,6 +3221,8 @@ static int ext4_rename(struct inode *old_dir, struct dentry *old_dentry,
 		dquot_initialize(new.inode);
 
 	old.bh = ext4_find_entry(old.dir, &amp;old.dentry-&gt;d_name, &amp;old.de, NULL);
+	if (IS_ERR(old.bh))
+		return PTR_ERR(old.bh);
 	/*
 	 *  Check for inode number is _not_ due to possible IO errors.
 	 *  We might rmdir the source, keep it as pwd of some process
@@ -3214,6 +3235,10 @@ static int ext4_rename(struct inode *old_dir, struct dentry *old_dentry,
 
 	new.bh = ext4_find_entry(new.dir, &amp;new.dentry-&gt;d_name,
 				 &amp;new.de, &amp;new.inlined);
+	if (IS_ERR(new.bh)) {
+		retval = PTR_ERR(new.bh);
+		goto end_rename;
+	}
 	if (new.bh) {
 		if (!new.inode) {
 			brelse(new.bh);
@@ -3330,6 +3355,8 @@ static int ext4_cross_rename(struct inode *old_dir, struct dentry *old_dentry,
 
 	old.bh = ext4_find_entry(old.dir, &amp;old.dentry-&gt;d_name,
 				 &amp;old.de, &amp;old.inlined);
+	if (IS_ERR(old.bh))
+		return PTR_ERR(old.bh);
 	/*
 	 *  Check for inode number is _not_ due to possible IO errors.
 	 *  We might rmdir the source, keep it as pwd of some process
@@ -3342,6 +3369,10 @@ static int ext4_cross_rename(struct inode *old_dir, struct dentry *old_dentry,
 
 	new.bh = ext4_find_entry(new.dir, &amp;new.dentry-&gt;d_name,
 				 &amp;new.de, &amp;new.inlined);
+	if (IS_ERR(new.bh)) {
+		retval = PTR_ERR(new.bh);
+		goto end_rename;
+	}
 
 	/* RENAME_EXCHANGE case: old *and* new must both exist */
 	if (!new.bh || le32_to_cpu(new.de-&gt;inode) != new.inode-&gt;i_ino)</pre><hr><pre>commit 48d6be955a7167b0d0e025ae6c39e795e3544499
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Jul 17 05:27:30 2014 -0400

    random: limit the contribution of the hw rng to at most half
    
    For people who don't trust a hardware RNG which can not be audited,
    the changes to add support for RDSEED can be troubling since 97% or
    more of the entropy will be contributed from the in-CPU hardware RNG.
    
    We now have a in-kernel khwrngd, so for those people who do want to
    implicitly trust the CPU-based system, we could create an arch-rng
    hw_random driver, and allow khwrng refill the entropy pool.  This
    allows system administrator whether or not they trust the CPU (I
    assume the NSA will trust RDRAND/RDSEED implicitly :-), and if so,
    what level of entropy derating they want to use.
    
    The reason why this is a really good idea is that if different people
    use different levels of entropy derating, it will make it much more
    difficult to design a backdoor'ed hwrng that can be generally
    exploited in terms of the output of /dev/random when different attack
    targets are using differing levels of entropy derating.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index 7d1682ea1e86..6e455bc4a39e 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -910,12 +910,13 @@ void add_interrupt_randomness(int irq, int irq_flags)
 
 	/*
 	 * If we have architectural seed generator, produce a seed and
-	 * add it to the pool.  For the sake of paranoia count it as
-	 * 50% entropic.
+	 * add it to the pool.  For the sake of paranoia don't let the
+	 * architectural seed generator dominate the input from the
+	 * interrupt noise.
 	 */
 	if (arch_get_random_seed_long(&amp;seed)) {
 		__mix_pool_bytes(r, &amp;seed, sizeof(seed));
-		credit += sizeof(seed) * 4;
+		credit = 1;
 	}
 	spin_unlock(&amp;r-&gt;lock);
 
@@ -1328,37 +1329,6 @@ void rand_initialize_disk(struct gendisk *disk)
 }
 #endif
 
-/*
- * Attempt an emergency refill using arch_get_random_seed_long().
- *
- * As with add_interrupt_randomness() be paranoid and only
- * credit the output as 50% entropic.
- */
-static int arch_random_refill(void)
-{
-	const unsigned int nlongs = 64;	/* Arbitrary number */
-	unsigned int n = 0;
-	unsigned int i;
-	unsigned long buf[nlongs];
-
-	if (!arch_has_random_seed())
-		return 0;
-
-	for (i = 0; i &lt; nlongs; i++) {
-		if (arch_get_random_seed_long(&amp;buf[n]))
-			n++;
-	}
-
-	if (n) {
-		unsigned int rand_bytes = n * sizeof(unsigned long);
-
-		mix_pool_bytes(&amp;input_pool, buf, rand_bytes);
-		credit_entropy_bits(&amp;input_pool, rand_bytes*4);
-	}
-
-	return n;
-}
-
 static ssize_t
 _random_read(int nonblock, char __user *buf, size_t nbytes)
 {
@@ -1379,11 +1349,6 @@ _random_read(int nonblock, char __user *buf, size_t nbytes)
 			return n;
 
 		/* Pool is (near) empty.  Maybe wait and retry. */
-
-		/* First try an emergency refill */
-		if (arch_random_refill())
-			continue;
-
 		if (nonblock)
 			return -EAGAIN;
 </pre><hr><pre>commit c6e9d6f38894798696f23c8084ca7edbf16ee895
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Jul 17 04:13:05 2014 -0400

    random: introduce getrandom(2) system call
    
    The getrandom(2) system call was requested by the LibreSSL Portable
    developers.  It is analoguous to the getentropy(2) system call in
    OpenBSD.
    
    The rationale of this system call is to provide resiliance against
    file descriptor exhaustion attacks, where the attacker consumes all
    available file descriptors, forcing the use of the fallback code where
    /dev/[u]random is not available.  Since the fallback code is often not
    well-tested, it is better to eliminate this potential failure mode
    entirely.
    
    The other feature provided by this new system call is the ability to
    request randomness from the /dev/urandom entropy pool, but to block
    until at least 128 bits of entropy has been accumulated in the
    /dev/urandom entropy pool.  Historically, the emphasis in the
    /dev/urandom development has been to ensure that urandom pool is
    initialized as quickly as possible after system boot, and preferably
    before the init scripts start execution.
    
    This is because changing /dev/urandom reads to block represents an
    interface change that could potentially break userspace which is not
    acceptable.  In practice, on most x86 desktop and server systems, in
    general the entropy pool can be initialized before it is needed (and
    in modern kernels, we will printk a warning message if not).  However,
    on an embedded system, this may not be the case.  And so with this new
    interface, we can provide the functionality of blocking until the
    urandom pool has been initialized.  Any userspace program which uses
    this new functionality must take care to assure that if it is used
    during the boot process, that it will not cause the init scripts or
    other portions of the system startup to hang indefinitely.
    
    SYNOPSIS
            #include &lt;linux/random.h&gt;
    
            int getrandom(void *buf, size_t buflen, unsigned int flags);
    
    DESCRIPTION
            The system call getrandom() fills the buffer pointed to by buf
            with up to buflen random bytes which can be used to seed user
            space random number generators (i.e., DRBG's) or for other
            cryptographic uses.  It should not be used for Monte Carlo
            simulations or other programs/algorithms which are doing
            probabilistic sampling.
    
            If the GRND_RANDOM flags bit is set, then draw from the
            /dev/random pool instead of the /dev/urandom pool.  The
            /dev/random pool is limited based on the entropy that can be
            obtained from environmental noise, so if there is insufficient
            entropy, the requested number of bytes may not be returned.
            If there is no entropy available at all, getrandom(2) will
            either block, or return an error with errno set to EAGAIN if
            the GRND_NONBLOCK bit is set in flags.
    
            If the GRND_RANDOM bit is not set, then the /dev/urandom pool
            will be used.  Unlike using read(2) to fetch data from
            /dev/urandom, if the urandom pool has not been sufficiently
            initialized, getrandom(2) will block (or return -1 with the
            errno set to EAGAIN if the GRND_NONBLOCK bit is set in flags).
    
            The getentropy(2) system call in OpenBSD can be emulated using
            the following function:
    
                int getentropy(void *buf, size_t buflen)
                {
                        int     ret;
    
                        if (buflen &gt; 256)
                                goto failure;
                        ret = getrandom(buf, buflen, 0);
                        if (ret &lt; 0)
                                return ret;
                        if (ret == buflen)
                                return 0;
                failure:
                        errno = EIO;
                        return -1;
                }
    
    RETURN VALUE
           On success, the number of bytes that was filled in the buf is
           returned.  This may not be all the bytes requested by the
           caller via buflen if insufficient entropy was present in the
           /dev/random pool, or if the system call was interrupted by a
           signal.
    
           On error, -1 is returned, and errno is set appropriately.
    
    ERRORS
            EINVAL          An invalid flag was passed to getrandom(2)
    
            EFAULT          buf is outside the accessible address space.
    
            EAGAIN          The requested entropy was not available, and
                            getentropy(2) would have blocked if the
                            GRND_NONBLOCK flag was not set.
    
            EINTR           While blocked waiting for entropy, the call was
                            interrupted by a signal handler; see the description
                            of how interrupted read(2) calls on "slow" devices
                            are handled with and without the SA_RESTART flag
                            in the signal(7) man page.
    
    NOTES
            For small requests (buflen &lt;= 256) getrandom(2) will not
            return EINTR when reading from the urandom pool once the
            entropy pool has been initialized, and it will return all of
            the bytes that have been requested.  This is the recommended
            way to use getrandom(2), and is designed for compatibility
            with OpenBSD's getentropy() system call.
    
            However, if you are using GRND_RANDOM, then getrandom(2) may
            block until the entropy accounting determines that sufficient
            environmental noise has been gathered such that getrandom(2)
            will be operating as a NRBG instead of a DRBG for those people
            who are working in the NIST SP 800-90 regime.  Since it may
            block for a long time, these guarantees do *not* apply.  The
            user may want to interrupt a hanging process using a signal,
            so blocking until all of the requested bytes are returned
            would be unfriendly.
    
            For this reason, the user of getrandom(2) MUST always check
            the return value, in case it returns some error, or if fewer
            bytes than requested was returned.  In the case of
            !GRND_RANDOM and small request, the latter should never
            happen, but the careful userspace code (and all crypto code
            should be careful) should check for this anyway!
    
            Finally, unless you are doing long-term key generation (and
            perhaps not even then), you probably shouldn't be using
            GRND_RANDOM.  The cryptographic algorithms used for
            /dev/urandom are quite conservative, and so should be
            sufficient for all purposes.  The disadvantage of GRND_RANDOM
            is that it can block, and the increased complexity required to
            deal with partially fulfilled getrandom(2) requests.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Zach Brown &lt;zab@zabbo.net&gt;

diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index d6b867921612..5b46a618aeb1 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -360,3 +360,4 @@
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
 353	i386	renameat2		sys_renameat2
+355	i386	getrandom		sys_getrandom
diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index ec255a1646d2..0dc4bf891460 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -323,6 +323,7 @@
 314	common	sched_setattr		sys_sched_setattr
 315	common	sched_getattr		sys_sched_getattr
 316	common	renameat2		sys_renameat2
+318	common	getrandom		sys_getrandom
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/drivers/char/random.c b/drivers/char/random.c
index aa22fe551c2a..7d1682ea1e86 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -258,6 +258,8 @@
 #include &lt;linux/kmemcheck.h&gt;
 #include &lt;linux/workqueue.h&gt;
 #include &lt;linux/irq.h&gt;
+#include &lt;linux/syscalls.h&gt;
+#include &lt;linux/completion.h&gt;
 
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/uaccess.h&gt;
@@ -404,6 +406,7 @@ static struct poolinfo {
  */
 static DECLARE_WAIT_QUEUE_HEAD(random_read_wait);
 static DECLARE_WAIT_QUEUE_HEAD(random_write_wait);
+static DECLARE_WAIT_QUEUE_HEAD(urandom_init_wait);
 static struct fasync_struct *fasync;
 
 /**********************************************************************
@@ -657,6 +660,7 @@ static void credit_entropy_bits(struct entropy_store *r, int nbits)
 		r-&gt;entropy_total = 0;
 		if (r == &amp;nonblocking_pool) {
 			prandom_reseed_late();
+			wake_up_interruptible(&amp;urandom_init_wait);
 			pr_notice("random: %s pool is initialized\n", r-&gt;name);
 		}
 	}
@@ -1174,13 +1178,14 @@ static ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,
 {
 	ssize_t ret = 0, i;
 	__u8 tmp[EXTRACT_SIZE];
+	int large_request = (nbytes &gt; 256);
 
 	trace_extract_entropy_user(r-&gt;name, nbytes, ENTROPY_BITS(r), _RET_IP_);
 	xfer_secondary_pool(r, nbytes);
 	nbytes = account(r, nbytes, 0, 0);
 
 	while (nbytes) {
-		if (need_resched()) {
+		if (large_request &amp;&amp; need_resched()) {
 			if (signal_pending(current)) {
 				if (ret == 0)
 					ret = -ERESTARTSYS;
@@ -1355,7 +1360,7 @@ static int arch_random_refill(void)
 }
 
 static ssize_t
-random_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)
+_random_read(int nonblock, char __user *buf, size_t nbytes)
 {
 	ssize_t n;
 
@@ -1379,7 +1384,7 @@ random_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)
 		if (arch_random_refill())
 			continue;
 
-		if (file-&gt;f_flags &amp; O_NONBLOCK)
+		if (nonblock)
 			return -EAGAIN;
 
 		wait_event_interruptible(random_read_wait,
@@ -1390,6 +1395,12 @@ random_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)
 	}
 }
 
+static ssize_t
+random_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)
+{
+	return _random_read(file-&gt;f_flags &amp; O_NONBLOCK, buf, nbytes);
+}
+
 static ssize_t
 urandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)
 {
@@ -1533,6 +1544,29 @@ const struct file_operations urandom_fops = {
 	.llseek = noop_llseek,
 };
 
+SYSCALL_DEFINE3(getrandom, char __user *, buf, size_t, count,
+		unsigned int, flags)
+{
+	if (flags &amp; ~(GRND_NONBLOCK|GRND_RANDOM))
+		return -EINVAL;
+
+	if (count &gt; INT_MAX)
+		count = INT_MAX;
+
+	if (flags &amp; GRND_RANDOM)
+		return _random_read(flags &amp; GRND_NONBLOCK, buf, count);
+
+	if (unlikely(nonblocking_pool.initialized == 0)) {
+		if (flags &amp; GRND_NONBLOCK)
+			return -EAGAIN;
+		wait_event_interruptible(urandom_init_wait,
+					 nonblocking_pool.initialized);
+		if (signal_pending(current))
+			return -ERESTARTSYS;
+	}
+	return urandom_read(NULL, buf, count, NULL);
+}
+
 /***************************************************************
  * Random UUID interface
  *
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index b0881a0ed322..43324a897cf2 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -866,4 +866,7 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage long sys_getrandom(char __user *buf, size_t count,
+			      unsigned int flags);
+
 #endif
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 333640608087..1d104a2ca643 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -699,9 +699,11 @@ __SYSCALL(__NR_sched_setattr, sys_sched_setattr)
 __SYSCALL(__NR_sched_getattr, sys_sched_getattr)
 #define __NR_renameat2 276
 __SYSCALL(__NR_renameat2, sys_renameat2)
+#define __NR_getrandom 278
+__SYSCALL(__NR_getrandom, sys_getrandom)
 
 #undef __NR_syscalls
-#define __NR_syscalls 277
+#define __NR_syscalls 279
 
 /*
  * All syscalls below here should go away really,
diff --git a/include/uapi/linux/random.h b/include/uapi/linux/random.h
index fff3528a078f..3f93d1695e7f 100644
--- a/include/uapi/linux/random.h
+++ b/include/uapi/linux/random.h
@@ -40,4 +40,13 @@ struct rand_pool_info {
 	__u32	buf[0];
 };
 
+/*
+ * Flags for getrandom(2)
+ *
+ * GRND_NONBLOCK	Don't block and return EAGAIN instead
+ * GRND_RANDOM		Use the /dev/random pool instead of /dev/urandom
+ */
+#define GRND_NONBLOCK	0x0001
+#define GRND_RANDOM	0x0002
+
 #endif /* _UAPI_LINUX_RANDOM_H */</pre><hr><pre>commit 86f0afd463215fc3e58020493482faa4ac3a4d69
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Jul 30 22:17:17 2014 -0400

    ext4: fix ext4_discard_allocated_blocks() if we can't allocate the pa struct
    
    If there is a failure while allocating the preallocation structure, a
    number of blocks can end up getting marked in the in-memory buddy
    bitmap, and then not getting released.  This can result in the
    following corruption getting reported by the kernel:
    
    EXT4-fs error (device sda3): ext4_mb_generate_buddy:758: group 1126,
    12793 clusters in bitmap, 12729 in gd
    
    In that case, we need to release the blocks using mb_free_blocks().
    
    Tested: fs smoke test; also demonstrated that with injected errors,
            the file system is no longer getting corrupted
    
    Google-Bug-Id: 16657874
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 0e9466f9e767..956027711faf 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -3217,8 +3217,27 @@ static void ext4_mb_collect_stats(struct ext4_allocation_context *ac)
 static void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)
 {
 	struct ext4_prealloc_space *pa = ac-&gt;ac_pa;
+	struct ext4_buddy e4b;
+	int err;
 
-	if (pa &amp;&amp; pa-&gt;pa_type == MB_INODE_PA)
+	if (pa == NULL) {
+		err = ext4_mb_load_buddy(ac-&gt;ac_sb, ac-&gt;ac_f_ex.fe_group, &amp;e4b);
+		if (err) {
+			/*
+			 * This should never happen since we pin the
+			 * pages in the ext4_allocation_context so
+			 * ext4_mb_load_buddy() should never fail.
+			 */
+			WARN(1, "mb_load_buddy failed (%d)", err);
+			return;
+		}
+		ext4_lock_group(ac-&gt;ac_sb, ac-&gt;ac_f_ex.fe_group);
+		mb_free_blocks(ac-&gt;ac_inode, &amp;e4b, ac-&gt;ac_f_ex.fe_start,
+			       ac-&gt;ac_f_ex.fe_len);
+		ext4_unlock_group(ac-&gt;ac_sb, ac-&gt;ac_f_ex.fe_group);
+		return;
+	}
+	if (pa-&gt;pa_type == MB_INODE_PA)
 		pa-&gt;pa_free += ac-&gt;ac_b_ex.fe_len;
 }
 </pre><hr><pre>commit 71d4f7d032149b935a26eb3ff85c6c837f3714e1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Tue Jul 15 06:02:38 2014 -0400

    ext4: remove metadata reservation checks
    
    Commit 27dd43854227b ("ext4: introduce reserved space") reserves 2% of
    the file system space to make sure metadata allocations will always
    succeed.  Given that, tracking the reservation of metadata blocks is
    no longer necessary.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index fca382037ddd..581ef40fbe90 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -639,7 +639,6 @@ ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,
 	if (!(*errp) &amp;&amp;
 	    ext4_test_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED)) {
 		spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
-		EXT4_I(inode)-&gt;i_allocated_meta_blocks += ar.len;
 		spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 		dquot_alloc_block_nofail(inode,
 				EXT4_C2B(EXT4_SB(inode-&gt;i_sb), ar.len));
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 7cc5a0e23688..d35c78c96184 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -591,7 +591,6 @@ enum {
 #define EXT4_FREE_BLOCKS_NO_QUOT_UPDATE	0x0008
 #define EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER	0x0010
 #define EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER	0x0020
-#define EXT4_FREE_BLOCKS_RESERVE		0x0040
 
 /*
  * ioctl commands
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index 4da228a0e6d0..b30172dd55eb 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -1808,8 +1808,7 @@ static void ext4_ext_try_to_merge_up(handle_t *handle,
 
 	brelse(path[1].p_bh);
 	ext4_free_blocks(handle, inode, NULL, blk, 1,
-			 EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET |
-			 EXT4_FREE_BLOCKS_RESERVE);
+			 EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);
 }
 
 /*
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 8a064734e6eb..027ee8c40470 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -324,18 +324,6 @@ qsize_t *ext4_get_reserved_space(struct inode *inode)
 }
 #endif
 
-/*
- * Calculate the number of metadata blocks need to reserve
- * to allocate a block located at @lblock
- */
-static int ext4_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)
-{
-	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
-		return ext4_ext_calc_metadata_amount(inode, lblock);
-
-	return ext4_ind_calc_metadata_amount(inode, lblock);
-}
-
 /*
  * Called with i_data_sem down, which is important since we can call
  * ext4_discard_preallocations() from here.
@@ -357,35 +345,10 @@ void ext4_da_update_reserve_space(struct inode *inode,
 		used = ei-&gt;i_reserved_data_blocks;
 	}
 
-	if (unlikely(ei-&gt;i_allocated_meta_blocks &gt; ei-&gt;i_reserved_meta_blocks)) {
-		ext4_warning(inode-&gt;i_sb, "ino %lu, allocated %d "
-			"with only %d reserved metadata blocks "
-			"(releasing %d blocks with reserved %d data blocks)",
-			inode-&gt;i_ino, ei-&gt;i_allocated_meta_blocks,
-			     ei-&gt;i_reserved_meta_blocks, used,
-			     ei-&gt;i_reserved_data_blocks);
-		WARN_ON(1);
-		ei-&gt;i_allocated_meta_blocks = ei-&gt;i_reserved_meta_blocks;
-	}
-
 	/* Update per-inode reservations */
 	ei-&gt;i_reserved_data_blocks -= used;
-	ei-&gt;i_reserved_meta_blocks -= ei-&gt;i_allocated_meta_blocks;
-	percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
-			   used + ei-&gt;i_allocated_meta_blocks);
-	ei-&gt;i_allocated_meta_blocks = 0;
+	percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter, used);
 
-	if (ei-&gt;i_reserved_data_blocks == 0) {
-		/*
-		 * We can release all of the reserved metadata blocks
-		 * only when we have written all of the delayed
-		 * allocation blocks.
-		 */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
-				   ei-&gt;i_reserved_meta_blocks);
-		ei-&gt;i_reserved_meta_blocks = 0;
-		ei-&gt;i_da_metadata_calc_len = 0;
-	}
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
 	/* Update quota subsystem for data blocks */
@@ -1221,49 +1184,6 @@ static int ext4_journalled_write_end(struct file *file,
 	return ret ? ret : copied;
 }
 
-/*
- * Reserve a metadata for a single block located at lblock
- */
-static int ext4_da_reserve_metadata(struct inode *inode, ext4_lblk_t lblock)
-{
-	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
-	struct ext4_inode_info *ei = EXT4_I(inode);
-	unsigned int md_needed;
-	ext4_lblk_t save_last_lblock;
-	int save_len;
-
-	/*
-	 * recalculate the amount of metadata blocks to reserve
-	 * in order to allocate nrblocks
-	 * worse case is one extent per block
-	 */
-	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
-	/*
-	 * ext4_calc_metadata_amount() has side effects, which we have
-	 * to be prepared undo if we fail to claim space.
-	 */
-	save_len = ei-&gt;i_da_metadata_calc_len;
-	save_last_lblock = ei-&gt;i_da_metadata_calc_last_lblock;
-	md_needed = EXT4_NUM_B2C(sbi,
-				 ext4_calc_metadata_amount(inode, lblock));
-	trace_ext4_da_reserve_space(inode, md_needed);
-
-	/*
-	 * We do still charge estimated metadata to the sb though;
-	 * we cannot afford to run out of free blocks.
-	 */
-	if (ext4_claim_free_clusters(sbi, md_needed, 0)) {
-		ei-&gt;i_da_metadata_calc_len = save_len;
-		ei-&gt;i_da_metadata_calc_last_lblock = save_last_lblock;
-		spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
-		return -ENOSPC;
-	}
-	ei-&gt;i_reserved_meta_blocks += md_needed;
-	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
-
-	return 0;       /* success */
-}
-
 /*
  * Reserve a single cluster located at lblock
  */
@@ -1273,8 +1193,6 @@ static int ext4_da_reserve_space(struct inode *inode, ext4_lblk_t lblock)
 	struct ext4_inode_info *ei = EXT4_I(inode);
 	unsigned int md_needed;
 	int ret;
-	ext4_lblk_t save_last_lblock;
-	int save_len;
 
 	/*
 	 * We will charge metadata quota at writeout time; this saves
@@ -1295,25 +1213,15 @@ static int ext4_da_reserve_space(struct inode *inode, ext4_lblk_t lblock)
 	 * ext4_calc_metadata_amount() has side effects, which we have
 	 * to be prepared undo if we fail to claim space.
 	 */
-	save_len = ei-&gt;i_da_metadata_calc_len;
-	save_last_lblock = ei-&gt;i_da_metadata_calc_last_lblock;
-	md_needed = EXT4_NUM_B2C(sbi,
-				 ext4_calc_metadata_amount(inode, lblock));
-	trace_ext4_da_reserve_space(inode, md_needed);
+	md_needed = 0;
+	trace_ext4_da_reserve_space(inode, 0);
 
-	/*
-	 * We do still charge estimated metadata to the sb though;
-	 * we cannot afford to run out of free blocks.
-	 */
-	if (ext4_claim_free_clusters(sbi, md_needed + 1, 0)) {
-		ei-&gt;i_da_metadata_calc_len = save_len;
-		ei-&gt;i_da_metadata_calc_last_lblock = save_last_lblock;
+	if (ext4_claim_free_clusters(sbi, 1, 0)) {
 		spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 		dquot_release_reservation_block(inode, EXT4_C2B(sbi, 1));
 		return -ENOSPC;
 	}
 	ei-&gt;i_reserved_data_blocks++;
-	ei-&gt;i_reserved_meta_blocks += md_needed;
 	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 
 	return 0;       /* success */
@@ -1346,20 +1254,6 @@ static void ext4_da_release_space(struct inode *inode, int to_free)
 	}
 	ei-&gt;i_reserved_data_blocks -= to_free;
 
-	if (ei-&gt;i_reserved_data_blocks == 0) {
-		/*
-		 * We can release all of the reserved metadata blocks
-		 * only when we have written all of the delayed
-		 * allocation blocks.
-		 * Note that in case of bigalloc, i_reserved_meta_blocks,
-		 * i_reserved_data_blocks, etc. refer to number of clusters.
-		 */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
-				   ei-&gt;i_reserved_meta_blocks);
-		ei-&gt;i_reserved_meta_blocks = 0;
-		ei-&gt;i_da_metadata_calc_len = 0;
-	}
-
 	/* update fs dirty data blocks counter */
 	percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter, to_free);
 
@@ -1500,10 +1394,6 @@ static void ext4_print_free_blocks(struct inode *inode)
 	ext4_msg(sb, KERN_CRIT, "Block reservation details");
 	ext4_msg(sb, KERN_CRIT, "i_reserved_data_blocks=%u",
 		 ei-&gt;i_reserved_data_blocks);
-	ext4_msg(sb, KERN_CRIT, "i_reserved_meta_blocks=%u",
-	       ei-&gt;i_reserved_meta_blocks);
-	ext4_msg(sb, KERN_CRIT, "i_allocated_meta_blocks=%u",
-	       ei-&gt;i_allocated_meta_blocks);
 	return;
 }
 
@@ -1620,13 +1510,6 @@ static int ext4_da_map_blocks(struct inode *inode, sector_t iblock,
 				retval = ret;
 				goto out_unlock;
 			}
-		} else {
-			ret = ext4_da_reserve_metadata(inode, iblock);
-			if (ret) {
-				/* not enough space to reserve */
-				retval = ret;
-				goto out_unlock;
-			}
 		}
 
 		ret = ext4_es_insert_extent(inode, map-&gt;m_lblk, map-&gt;m_len,
@@ -2843,8 +2726,7 @@ int ext4_alloc_da_blocks(struct inode *inode)
 {
 	trace_ext4_alloc_da_blocks(inode);
 
-	if (!EXT4_I(inode)-&gt;i_reserved_data_blocks &amp;&amp;
-	    !EXT4_I(inode)-&gt;i_reserved_meta_blocks)
+	if (!EXT4_I(inode)-&gt;i_reserved_data_blocks)
 		return 0;
 
 	/*
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 2dcb936be90e..18a16191249a 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -4627,7 +4627,6 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	struct buffer_head *gd_bh;
 	ext4_group_t block_group;
 	struct ext4_sb_info *sbi;
-	struct ext4_inode_info *ei = EXT4_I(inode);
 	struct ext4_buddy e4b;
 	unsigned int count_clusters;
 	int err = 0;
@@ -4838,19 +4837,7 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
-	if (flags &amp; EXT4_FREE_BLOCKS_RESERVE &amp;&amp; ei-&gt;i_reserved_data_blocks) {
-		percpu_counter_add(&amp;sbi-&gt;s_dirtyclusters_counter,
-				   count_clusters);
-		spin_lock(&amp;ei-&gt;i_block_reservation_lock);
-		if (flags &amp; EXT4_FREE_BLOCKS_METADATA)
-			ei-&gt;i_reserved_meta_blocks += count_clusters;
-		else
-			ei-&gt;i_reserved_data_blocks += count_clusters;
-		spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
-		if (!(flags &amp; EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
-			dquot_reclaim_block(inode,
-					EXT4_C2B(sbi, count_clusters));
-	} else if (!(flags &amp; EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
+	if (!(flags &amp; EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
 		dquot_free_block(inode, EXT4_C2B(sbi, count_clusters));
 	percpu_counter_add(&amp;sbi-&gt;s_freeclusters_counter, count_clusters);
 </pre><hr><pre>commit d5e03cbb0c88cd1be39f2adc37d602230045964b
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Tue Jul 15 06:01:38 2014 -0400

    ext4: rearrange initialization to fix EXT4FS_DEBUG
    
    The EXT4FS_DEBUG is a *very* developer specific #ifdef designed for
    ext4 developers only.  (You have to modify fs/ext4/ext4.h to enable
    it.)
    
    Rearrange how we initialize data structures to avoid calling
    ext4_count_free_clusters() until the multiblock allocator has been
    initialized.
    
    This also allows us to only call ext4_count_free_clusters() once, and
    simplifies the code somewhat.
    
    (Thanks to Chen Gang &lt;gang.chen.5i5j@gmail.com&gt; for pointing out a
    !CONFIG_SMP compile breakage in the original patch.)
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Lukas Czerner &lt;lczerner@redhat.com&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 6df7bc611dbd..32b43ad154b9 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -2142,10 +2142,6 @@ static int ext4_check_descriptors(struct super_block *sb,
 	}
 	if (NULL != first_not_zeroed)
 		*first_not_zeroed = grp;
-
-	ext4_free_blocks_count_set(sbi-&gt;s_es,
-				   EXT4_C2B(sbi, ext4_count_free_clusters(sb)));
-	sbi-&gt;s_es-&gt;s_free_inodes_count =cpu_to_le32(ext4_count_free_inodes(sb));
 	return 1;
 }
 
@@ -3883,13 +3879,6 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 		ext4_msg(sb, KERN_ERR, "group descriptors corrupted!");
 		goto failed_mount2;
 	}
-	if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))
-		if (!ext4_fill_flex_info(sb)) {
-			ext4_msg(sb, KERN_ERR,
-			       "unable to initialize "
-			       "flex_bg meta info!");
-			goto failed_mount2;
-		}
 
 	sbi-&gt;s_gdb_count = db_count;
 	get_random_bytes(&amp;sbi-&gt;s_next_generation, sizeof(u32));
@@ -3902,23 +3891,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	/* Register extent status tree shrinker */
 	ext4_es_register_shrinker(sbi);
 
-	err = percpu_counter_init(&amp;sbi-&gt;s_freeclusters_counter,
-			ext4_count_free_clusters(sb));
-	if (!err) {
-		err = percpu_counter_init(&amp;sbi-&gt;s_freeinodes_counter,
-				ext4_count_free_inodes(sb));
-	}
-	if (!err) {
-		err = percpu_counter_init(&amp;sbi-&gt;s_dirs_counter,
-				ext4_count_dirs(sb));
-	}
-	if (!err) {
-		err = percpu_counter_init(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
-	}
-	if (!err) {
-		err = percpu_counter_init(&amp;sbi-&gt;s_extent_cache_cnt, 0);
-	}
-	if (err) {
+	if ((err = percpu_counter_init(&amp;sbi-&gt;s_extent_cache_cnt, 0)) != 0) {
 		ext4_msg(sb, KERN_ERR, "insufficient memory");
 		goto failed_mount3;
 	}
@@ -4022,18 +3995,6 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 
 	sbi-&gt;s_journal-&gt;j_commit_callback = ext4_journal_commit_callback;
 
-	/*
-	 * The journal may have updated the bg summary counts, so we
-	 * need to update the global counters.
-	 */
-	percpu_counter_set(&amp;sbi-&gt;s_freeclusters_counter,
-			   ext4_count_free_clusters(sb));
-	percpu_counter_set(&amp;sbi-&gt;s_freeinodes_counter,
-			   ext4_count_free_inodes(sb));
-	percpu_counter_set(&amp;sbi-&gt;s_dirs_counter,
-			   ext4_count_dirs(sb));
-	percpu_counter_set(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
-
 no_journal:
 	if (ext4_mballoc_ready) {
 		sbi-&gt;s_mb_cache = ext4_xattr_create_cache(sb-&gt;s_id);
@@ -4141,6 +4102,33 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 		goto failed_mount5;
 	}
 
+	block = ext4_count_free_clusters(sb);
+	ext4_free_blocks_count_set(sbi-&gt;s_es, 
+				   EXT4_C2B(sbi, block));
+	err = percpu_counter_init(&amp;sbi-&gt;s_freeclusters_counter, block);
+	if (!err) {
+		unsigned long freei = ext4_count_free_inodes(sb);
+		sbi-&gt;s_es-&gt;s_free_inodes_count = cpu_to_le32(freei);
+		err = percpu_counter_init(&amp;sbi-&gt;s_freeinodes_counter, freei);
+	}
+	if (!err)
+		err = percpu_counter_init(&amp;sbi-&gt;s_dirs_counter,
+					  ext4_count_dirs(sb));
+	if (!err)
+		err = percpu_counter_init(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
+	if (err) {
+		ext4_msg(sb, KERN_ERR, "insufficient memory");
+		goto failed_mount6;
+	}
+
+	if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG))
+		if (!ext4_fill_flex_info(sb)) {
+			ext4_msg(sb, KERN_ERR,
+			       "unable to initialize "
+			       "flex_bg meta info!");
+			goto failed_mount6;
+		}
+
 	err = ext4_register_li_request(sb, first_not_zeroed);
 	if (err)
 		goto failed_mount6;
@@ -4215,6 +4203,12 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	ext4_unregister_li_request(sb);
 failed_mount6:
 	ext4_mb_release(sb);
+	if (sbi-&gt;s_flex_groups)
+		ext4_kvfree(sbi-&gt;s_flex_groups);
+	percpu_counter_destroy(&amp;sbi-&gt;s_freeclusters_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
 failed_mount5:
 	ext4_ext_release(sb);
 	ext4_release_system_zone(sb);
@@ -4233,12 +4227,6 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 failed_mount3:
 	ext4_es_unregister_shrinker(sbi);
 	del_timer_sync(&amp;sbi-&gt;s_err_report);
-	if (sbi-&gt;s_flex_groups)
-		ext4_kvfree(sbi-&gt;s_flex_groups);
-	percpu_counter_destroy(&amp;sbi-&gt;s_freeclusters_counter);
-	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
-	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
-	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_extent_cache_cnt);
 	if (sbi-&gt;s_mmp_tsk)
 		kthread_stop(sbi-&gt;s_mmp_tsk);
@@ -4556,11 +4544,13 @@ static int ext4_commit_super(struct super_block *sb, int sync)
 	else
 		es-&gt;s_kbytes_written =
 			cpu_to_le64(EXT4_SB(sb)-&gt;s_kbytes_written);
-	ext4_free_blocks_count_set(es,
+	if (percpu_counter_initialized(&amp;EXT4_SB(sb)-&gt;s_freeclusters_counter))
+		ext4_free_blocks_count_set(es,
 			EXT4_C2B(EXT4_SB(sb), percpu_counter_sum_positive(
 				&amp;EXT4_SB(sb)-&gt;s_freeclusters_counter)));
-	es-&gt;s_free_inodes_count =
-		cpu_to_le32(percpu_counter_sum_positive(
+	if (percpu_counter_initialized(&amp;EXT4_SB(sb)-&gt;s_freeinodes_counter))
+		es-&gt;s_free_inodes_count =
+			cpu_to_le32(percpu_counter_sum_positive(
 				&amp;EXT4_SB(sb)-&gt;s_freeinodes_counter));
 	BUFFER_TRACE(sbh, "marking dirty");
 	ext4_superblock_csum_set(sb);</pre><hr><pre>commit ee3e00e9e7101c80a2ff2d5672d4b486bf001b88
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Jun 15 16:59:24 2014 -0400

    random: use registers from interrupted code for CPU's w/o a cycle counter
    
    For CPU's that don't have a cycle counter, or something equivalent
    which can be used for random_get_entropy(), random_get_entropy() will
    always return 0.  In that case, substitute with the saved interrupt
    registers to add a bit more unpredictability.
    
    Some folks have suggested hashing all of the registers
    unconditionally, but this would increase the overhead of
    add_interrupt_randomness() by at least an order of magnitude, and this
    would very likely be unacceptable.
    
    The changes in this commit have been benchmarked as mostly unaffecting
    the overhead of add_interrupt_randomness() if the entropy counter is
    present, and doubling the overhead if it is not present.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: JÃ¶rn Engel &lt;joern@logfs.org&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index 914b1575df8f..aa22fe551c2a 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -551,9 +551,8 @@ static void mix_pool_bytes(struct entropy_store *r, const void *in,
 struct fast_pool {
 	__u32		pool[4];
 	unsigned long	last;
+	unsigned short	reg_idx;
 	unsigned char	count;
-	unsigned char	notimer_count;
-	unsigned char	rotate;
 };
 
 /*
@@ -857,6 +856,17 @@ static void add_interrupt_bench(cycles_t start)
 #define add_interrupt_bench(x)
 #endif
 
+static __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)
+{
+	__u32 *ptr = (__u32 *) regs;
+
+	if (regs == NULL)
+		return 0;
+	if (f-&gt;reg_idx &gt;= sizeof(struct pt_regs) / sizeof(__u32))
+		f-&gt;reg_idx = 0;
+	return *(ptr + f-&gt;reg_idx++);
+}
+
 void add_interrupt_randomness(int irq, int irq_flags)
 {
 	struct entropy_store	*r;
@@ -869,28 +879,23 @@ void add_interrupt_randomness(int irq, int irq_flags)
 	unsigned long		seed;
 	int			credit = 0;
 
+	if (cycles == 0)
+		cycles = get_reg(fast_pool, regs);
 	c_high = (sizeof(cycles) &gt; 4) ? cycles &gt;&gt; 32 : 0;
 	j_high = (sizeof(now) &gt; 4) ? now &gt;&gt; 32 : 0;
 	fast_pool-&gt;pool[0] ^= cycles ^ j_high ^ irq;
 	fast_pool-&gt;pool[1] ^= now ^ c_high;
 	ip = regs ? instruction_pointer(regs) : _RET_IP_;
 	fast_pool-&gt;pool[2] ^= ip;
-	fast_pool-&gt;pool[3] ^= ip &gt;&gt; 32;
+	fast_pool-&gt;pool[3] ^= (sizeof(ip) &gt; 4) ? ip &gt;&gt; 32 :
+		get_reg(fast_pool, regs);
 
 	fast_mix(fast_pool);
-	if ((irq_flags &amp; __IRQF_TIMER) == 0)
-		fast_pool-&gt;notimer_count++;
 	add_interrupt_bench(cycles);
 
-	if (cycles) {
-		if ((fast_pool-&gt;count &lt; 64) &amp;&amp;
-		    !time_after(now, fast_pool-&gt;last + HZ))
-			return;
-	} else {
-		/* CPU does not have a cycle counting register :-( */
-		if (fast_pool-&gt;count &lt; 64)
-			return;
-	}
+	if ((fast_pool-&gt;count &lt; 64) &amp;&amp;
+	    !time_after(now, fast_pool-&gt;last + HZ))
+		return;
 
 	r = nonblocking_pool.initialized ? &amp;input_pool : &amp;nonblocking_pool;
 	if (!spin_trylock(&amp;r-&gt;lock))
@@ -910,18 +915,10 @@ void add_interrupt_randomness(int irq, int irq_flags)
 	}
 	spin_unlock(&amp;r-&gt;lock);
 
-	/*
-	 * If we have a valid cycle counter or if the majority of
-	 * interrupts collected were non-timer interrupts, then give
-	 * an entropy credit of 1 bit.  Yes, this is being very
-	 * conservative.
-	 */
-	if (cycles || (fast_pool-&gt;notimer_count &gt;= 32))
-		credit++;
-
-	fast_pool-&gt;count = fast_pool-&gt;notimer_count = 0;
+	fast_pool-&gt;count = 0;
 
-	credit_entropy_bits(r, credit);
+	/* award one bit for the contents of the fast pool */
+	credit_entropy_bits(r, credit + 1);
 }
 
 #ifdef CONFIG_BLOCK</pre><hr><pre>commit 43759d4f429c8d55fd56f863542e20f4e6e8f589
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Jun 14 21:43:13 2014 -0400

    random: use an improved fast_mix() function
    
    Use more efficient fast_mix() function.  Thanks to George Spelvin for
    doing the leg work to find a more efficient mixing function.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: George Spelvin &lt;linux@horizon.com&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index dfe918a21e32..d3bb7927fb49 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -267,6 +267,8 @@
 #define CREATE_TRACE_POINTS
 #include &lt;trace/events/random.h&gt;
 
+/* #define ADD_INTERRUPT_BENCH */
+
 /*
  * Configuration information
  */
@@ -558,25 +560,29 @@ struct fast_pool {
  * collector.  It's hardcoded for an 128 bit pool and assumes that any
  * locks that might be needed are taken by the caller.
  */
-static void fast_mix(struct fast_pool *f, __u32 input[4])
+static void fast_mix(struct fast_pool *f)
 {
-	__u32		w;
-	unsigned	input_rotate = f-&gt;rotate;
-
-	w = rol32(input[0], input_rotate) ^ f-&gt;pool[0] ^ f-&gt;pool[3];
-	f-&gt;pool[0] = (w &gt;&gt; 3) ^ twist_table[w &amp; 7];
-	input_rotate = (input_rotate + 14) &amp; 31;
-	w = rol32(input[1], input_rotate) ^ f-&gt;pool[1] ^ f-&gt;pool[0];
-	f-&gt;pool[1] = (w &gt;&gt; 3) ^ twist_table[w &amp; 7];
-	input_rotate = (input_rotate + 7) &amp; 31;
-	w = rol32(input[2], input_rotate) ^ f-&gt;pool[2] ^ f-&gt;pool[1];
-	f-&gt;pool[2] = (w &gt;&gt; 3) ^ twist_table[w &amp; 7];
-	input_rotate = (input_rotate + 7) &amp; 31;
-	w = rol32(input[3], input_rotate) ^ f-&gt;pool[3] ^ f-&gt;pool[2];
-	f-&gt;pool[3] = (w &gt;&gt; 3) ^ twist_table[w &amp; 7];
-	input_rotate = (input_rotate + 7) &amp; 31;
-
-	f-&gt;rotate = input_rotate;
+	__u32 a = f-&gt;pool[0],	b = f-&gt;pool[1];
+	__u32 c = f-&gt;pool[2],	d = f-&gt;pool[3];
+
+	a += b;			c += d;
+	b = rol32(a, 6);	d = rol32(c, 27);
+	d ^= a;			b ^= c;
+
+	a += b;			c += d;
+	b = rol32(a, 16);	d = rol32(c, 14);
+	d ^= a;			b ^= c;
+
+	a += b;			c += d;
+	b = rol32(a, 6);	d = rol32(c, 27);
+	d ^= a;			b ^= c;
+
+	a += b;			c += d;
+	b = rol32(a, 16);	d = rol32(c, 14);
+	d ^= a;			b ^= c;
+
+	f-&gt;pool[0] = a;  f-&gt;pool[1] = b;
+	f-&gt;pool[2] = c;  f-&gt;pool[3] = d;
 	f-&gt;count++;
 }
 
@@ -829,6 +835,27 @@ EXPORT_SYMBOL_GPL(add_input_randomness);
 
 static DEFINE_PER_CPU(struct fast_pool, irq_randomness);
 
+#ifdef ADD_INTERRUPT_BENCH
+static unsigned long avg_cycles, avg_deviation;
+
+#define AVG_SHIFT 8     /* Exponential average factor k=1/256 */
+#define FIXED_1_2 (1 &lt;&lt; (AVG_SHIFT-1))
+
+static void add_interrupt_bench(cycles_t start)
+{
+        long delta = random_get_entropy() - start;
+
+        /* Use a weighted moving average */
+        delta = delta - ((avg_cycles + FIXED_1_2) &gt;&gt; AVG_SHIFT);
+        avg_cycles += delta;
+        /* And average deviation */
+        delta = abs(delta) - ((avg_deviation + FIXED_1_2) &gt;&gt; AVG_SHIFT);
+        avg_deviation += delta;
+}
+#else
+#define add_interrupt_bench(x)
+#endif
+
 void add_interrupt_randomness(int irq, int irq_flags)
 {
 	struct entropy_store	*r;
@@ -836,22 +863,23 @@ void add_interrupt_randomness(int irq, int irq_flags)
 	struct pt_regs		*regs = get_irq_regs();
 	unsigned long		now = jiffies;
 	cycles_t		cycles = random_get_entropy();
-	__u32			input[4], c_high, j_high;
+	__u32			c_high, j_high;
 	__u64			ip;
 	unsigned long		seed;
 	int			credit = 0;
 
 	c_high = (sizeof(cycles) &gt; 4) ? cycles &gt;&gt; 32 : 0;
 	j_high = (sizeof(now) &gt; 4) ? now &gt;&gt; 32 : 0;
-	input[0] = cycles ^ j_high ^ irq;
-	input[1] = now ^ c_high;
+	fast_pool-&gt;pool[0] ^= cycles ^ j_high ^ irq;
+	fast_pool-&gt;pool[1] ^= now ^ c_high;
 	ip = regs ? instruction_pointer(regs) : _RET_IP_;
-	input[2] = ip;
-	input[3] = ip &gt;&gt; 32;
+	fast_pool-&gt;pool[2] ^= ip;
+	fast_pool-&gt;pool[3] ^= ip &gt;&gt; 32;
 
-	fast_mix(fast_pool, input);
+	fast_mix(fast_pool);
 	if ((irq_flags &amp; __IRQF_TIMER) == 0)
 		fast_pool-&gt;notimer_count++;
+	add_interrupt_bench(cycles);
 
 	if (cycles) {
 		if ((fast_pool-&gt;count &lt; 64) &amp;&amp;
@@ -1650,6 +1678,22 @@ struct ctl_table random_table[] = {
 		.mode		= 0444,
 		.proc_handler	= proc_do_uuid,
 	},
+#ifdef ADD_INTERRUPT_BENCH
+	{
+		.procname	= "add_interrupt_avg_cycles",
+		.data		= &amp;avg_cycles,
+		.maxlen		= sizeof(avg_cycles),
+		.mode		= 0444,
+		.proc_handler	= proc_doulongvec_minmax,
+	},
+	{
+		.procname	= "add_interrupt_avg_deviation",
+		.data		= &amp;avg_deviation,
+		.maxlen		= sizeof(avg_deviation),
+		.mode		= 0444,
+		.proc_handler	= proc_doulongvec_minmax,
+	},
+#endif
 	{ }
 };
 #endif 	/* CONFIG_SYSCTL */</pre><hr><pre>commit 840f95077ffd640df9c74ad9796fa094a5c8075a
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Jun 14 03:06:57 2014 -0400

    random: clean up interrupt entropy accounting for archs w/o cycle counters
    
    For architectures that don't have cycle counters, the algorithm for
    deciding when to avoid giving entropy credit due to back-to-back timer
    interrupts didn't make any sense, since we were checking every 64
    interrupts.  Change it so that we only give an entropy credit if the
    majority of the interrupts are not based on the timer.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: George Spelvin &lt;linux@horizon.com&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index 364a8001a2bd..dfe918a21e32 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -548,9 +548,9 @@ static void mix_pool_bytes(struct entropy_store *r, const void *in,
 struct fast_pool {
 	__u32		pool[4];
 	unsigned long	last;
-	unsigned short	count;
+	unsigned char	count;
+	unsigned char	notimer_count;
 	unsigned char	rotate;
-	unsigned char	last_timer_intr;
 };
 
 /*
@@ -850,15 +850,23 @@ void add_interrupt_randomness(int irq, int irq_flags)
 	input[3] = ip &gt;&gt; 32;
 
 	fast_mix(fast_pool, input);
+	if ((irq_flags &amp; __IRQF_TIMER) == 0)
+		fast_pool-&gt;notimer_count++;
 
-	if ((fast_pool-&gt;count &amp; 63) &amp;&amp; !time_after(now, fast_pool-&gt;last + HZ))
-		return;
+	if (cycles) {
+		if ((fast_pool-&gt;count &lt; 64) &amp;&amp;
+		    !time_after(now, fast_pool-&gt;last + HZ))
+			return;
+	} else {
+		/* CPU does not have a cycle counting register :-( */
+		if (fast_pool-&gt;count &lt; 64)
+			return;
+	}
 
 	r = nonblocking_pool.initialized ? &amp;input_pool : &amp;nonblocking_pool;
-	if (!spin_trylock(&amp;r-&gt;lock)) {
-		fast_pool-&gt;count--;
+	if (!spin_trylock(&amp;r-&gt;lock))
 		return;
-	}
+
 	fast_pool-&gt;last = now;
 	__mix_pool_bytes(r, &amp;fast_pool-&gt;pool, sizeof(fast_pool-&gt;pool));
 
@@ -874,19 +882,15 @@ void add_interrupt_randomness(int irq, int irq_flags)
 	spin_unlock(&amp;r-&gt;lock);
 
 	/*
-	 * If we don't have a valid cycle counter, and we see
-	 * back-to-back timer interrupts, then skip giving credit for
-	 * any entropy, otherwise credit 1 bit.
+	 * If we have a valid cycle counter or if the majority of
+	 * interrupts collected were non-timer interrupts, then give
+	 * an entropy credit of 1 bit.  Yes, this is being very
+	 * conservative.
 	 */
-	credit++;
-	if (cycles == 0) {
-		if (irq_flags &amp; __IRQF_TIMER) {
-			if (fast_pool-&gt;last_timer_intr)
-				credit--;
-			fast_pool-&gt;last_timer_intr = 1;
-		} else
-			fast_pool-&gt;last_timer_intr = 0;
-	}
+	if (cycles || (fast_pool-&gt;notimer_count &gt;= 32))
+		credit++;
+
+	fast_pool-&gt;count = fast_pool-&gt;notimer_count = 0;
 
 	credit_entropy_bits(r, credit);
 }</pre>
    <div class="pagination">
        <a href='1_38.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><span>[39]</span><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_40.html'>Next&gt;&gt;</a>
    <div>
</body>
