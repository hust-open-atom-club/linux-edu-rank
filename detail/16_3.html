<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by California Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by California Institute of Technology</h1>
    <div class="pagination">
        <a href='16_2.html'>&lt;&lt;Prev</a><a href='16.html'>1</a><a href='16_2.html'>2</a><span>[3]</span><a href='16_4.html'>4</a><a href='16_5.html'>5</a><a href='16_6.html'>6</a><a href='16_7.html'>7</a><a href='16_4.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 9c4d1e7bdeb1ed4dc0c3341d40662a6fbc5f2dc2
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:59 2011 +0000

    fsldma: support async_tx dependencies and automatic unmapping
    
    Previous to this patch, the dma_run_dependencies() function has been
    called while holding desc_lock. This function can call tx_submit() for
    other descriptors, which may try to re-grab the lock. Avoid this by
    moving the descriptors to be cleaned up to a temporary list, and
    dropping the lock before cleanup.
    
    At the same time, add support for automatic unmapping of src and dst
    buffers, as offered by the DMAEngine API.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 6e9ad6edc4af..526579df6033 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -83,6 +83,11 @@ static void set_desc_cnt(struct fsldma_chan *chan,
 	hw-&gt;count = CPU_TO_DMA(chan, count, 32);
 }
 
+static u32 get_desc_cnt(struct fsldma_chan *chan, struct fsl_desc_sw *desc)
+{
+	return DMA_TO_CPU(chan, desc-&gt;hw.count, 32);
+}
+
 static void set_desc_src(struct fsldma_chan *chan,
 			 struct fsl_dma_ld_hw *hw, dma_addr_t src)
 {
@@ -93,6 +98,16 @@ static void set_desc_src(struct fsldma_chan *chan,
 	hw-&gt;src_addr = CPU_TO_DMA(chan, snoop_bits | src, 64);
 }
 
+static dma_addr_t get_desc_src(struct fsldma_chan *chan,
+			       struct fsl_desc_sw *desc)
+{
+	u64 snoop_bits;
+
+	snoop_bits = ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)
+		? ((u64)FSL_DMA_SATR_SREADTYPE_SNOOP_READ &lt;&lt; 32) : 0;
+	return DMA_TO_CPU(chan, desc-&gt;hw.src_addr, 64) &amp; ~snoop_bits;
+}
+
 static void set_desc_dst(struct fsldma_chan *chan,
 			 struct fsl_dma_ld_hw *hw, dma_addr_t dst)
 {
@@ -103,6 +118,16 @@ static void set_desc_dst(struct fsldma_chan *chan,
 	hw-&gt;dst_addr = CPU_TO_DMA(chan, snoop_bits | dst, 64);
 }
 
+static dma_addr_t get_desc_dst(struct fsldma_chan *chan,
+			       struct fsl_desc_sw *desc)
+{
+	u64 snoop_bits;
+
+	snoop_bits = ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)
+		? ((u64)FSL_DMA_DATR_DWRITETYPE_SNOOP_WRITE &lt;&lt; 32) : 0;
+	return DMA_TO_CPU(chan, desc-&gt;hw.dst_addr, 64) &amp; ~snoop_bits;
+}
+
 static void set_desc_next(struct fsldma_chan *chan,
 			  struct fsl_dma_ld_hw *hw, dma_addr_t next)
 {
@@ -805,6 +830,57 @@ static int fsl_dma_device_control(struct dma_chan *dchan,
 	return 0;
 }
 
+/**
+ * fsldma_cleanup_descriptor - cleanup and free a single link descriptor
+ * @chan: Freescale DMA channel
+ * @desc: descriptor to cleanup and free
+ *
+ * This function is used on a descriptor which has been executed by the DMA
+ * controller. It will run any callbacks, submit any dependencies, and then
+ * free the descriptor.
+ */
+static void fsldma_cleanup_descriptor(struct fsldma_chan *chan,
+				      struct fsl_desc_sw *desc)
+{
+	struct dma_async_tx_descriptor *txd = &amp;desc-&gt;async_tx;
+	struct device *dev = chan-&gt;common.device-&gt;dev;
+	dma_addr_t src = get_desc_src(chan, desc);
+	dma_addr_t dst = get_desc_dst(chan, desc);
+	u32 len = get_desc_cnt(chan, desc);
+
+	/* Run the link descriptor callback function */
+	if (txd-&gt;callback) {
+#ifdef FSL_DMA_LD_DEBUG
+		chan_dbg(chan, "LD %p callback\n", desc);
+#endif
+		txd-&gt;callback(txd-&gt;callback_param);
+	}
+
+	/* Run any dependencies */
+	dma_run_dependencies(txd);
+
+	/* Unmap the dst buffer, if requested */
+	if (!(txd-&gt;flags &amp; DMA_COMPL_SKIP_DEST_UNMAP)) {
+		if (txd-&gt;flags &amp; DMA_COMPL_DEST_UNMAP_SINGLE)
+			dma_unmap_single(dev, dst, len, DMA_FROM_DEVICE);
+		else
+			dma_unmap_page(dev, dst, len, DMA_FROM_DEVICE);
+	}
+
+	/* Unmap the src buffer, if requested */
+	if (!(txd-&gt;flags &amp; DMA_COMPL_SKIP_SRC_UNMAP)) {
+		if (txd-&gt;flags &amp; DMA_COMPL_SRC_UNMAP_SINGLE)
+			dma_unmap_single(dev, src, len, DMA_TO_DEVICE);
+		else
+			dma_unmap_page(dev, src, len, DMA_TO_DEVICE);
+	}
+
+#ifdef FSL_DMA_LD_DEBUG
+	chan_dbg(chan, "LD %p free\n", desc);
+#endif
+	dma_pool_free(chan-&gt;desc_pool, desc, txd-&gt;phys);
+}
+
 /**
  * fsl_chan_ld_cleanup - Clean up link descriptors
  * @chan : Freescale DMA channel
@@ -818,56 +894,39 @@ static int fsl_dma_device_control(struct dma_chan *dchan,
 static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 {
 	struct fsl_desc_sw *desc, *_desc;
+	LIST_HEAD(ld_cleanup);
 	unsigned long flags;
 
 	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
-	/* if the ld_running list is empty, there is nothing to do */
-	if (list_empty(&amp;chan-&gt;ld_running)) {
-		chan_dbg(chan, "no descriptors to cleanup\n");
-		goto out_unlock;
+	/* update the cookie if we have some descriptors to cleanup */
+	if (!list_empty(&amp;chan-&gt;ld_running)) {
+		dma_cookie_t cookie;
+
+		desc = to_fsl_desc(chan-&gt;ld_running.prev);
+		cookie = desc-&gt;async_tx.cookie;
+
+		chan-&gt;completed_cookie = cookie;
+		chan_dbg(chan, "completed cookie=%d\n", cookie);
 	}
 
 	/*
-	 * Get the last descriptor, update the cookie to it
-	 *
-	 * This is done before callbacks run so that clients can check the
-	 * status of their DMA transfer inside the callback.
+	 * move the descriptors to a temporary list so we can drop the lock
+	 * during the entire cleanup operation
 	 */
-	desc = to_fsl_desc(chan-&gt;ld_running.prev);
-	chan-&gt;completed_cookie = desc-&gt;async_tx.cookie;
-	chan_dbg(chan, "completed_cookie = %d\n", chan-&gt;completed_cookie);
+	list_splice_tail_init(&amp;chan-&gt;ld_running, &amp;ld_cleanup);
+
+	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
 
 	/* Run the callback for each descriptor, in order */
-	list_for_each_entry_safe(desc, _desc, &amp;chan-&gt;ld_running, node) {
-		dma_async_tx_callback callback;
-		void *callback_param;
+	list_for_each_entry_safe(desc, _desc, &amp;ld_cleanup, node) {
 
-		/* Remove from the list of running transactions */
+		/* Remove from the list of transactions */
 		list_del(&amp;desc-&gt;node);
 
-		/* Run the link descriptor callback function */
-		callback = desc-&gt;async_tx.callback;
-		callback_param = desc-&gt;async_tx.callback_param;
-		if (callback) {
-			spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
-#ifdef FSL_DMA_LD_DEBUG
-			chan_dbg(chan, "LD %p callback\n", desc);
-#endif
-			callback(callback_param);
-			spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
-		}
-
-		/* Run any dependencies, then free the descriptor */
-		dma_run_dependencies(&amp;desc-&gt;async_tx);
-#ifdef FSL_DMA_LD_DEBUG
-		chan_dbg(chan, "LD %p free\n", desc);
-#endif
-		dma_pool_free(chan-&gt;desc_pool, desc, desc-&gt;async_tx.phys);
+		/* Run all cleanup for this descriptor */
+		fsldma_cleanup_descriptor(chan, desc);
 	}
-
-out_unlock:
-	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
 }
 
 /**</pre><hr><pre>commit f04cd40701deace2efb9edd7120e59366bda2118
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:58 2011 +0000

    fsldma: fix controller lockups
    
    Enabling poisoning in the dmapool API quickly showed that the DMA
    controller was fetching descriptors that should not have been in use.
    This has caused intermittent controller lockups during testing.
    
    I have been unable to figure out the exact set of conditions which cause
    this to happen. However, I believe it is related to the driver using the
    hardware registers to track whether the controller is busy or not. The
    code can incorrectly decide that the hardware is idle due to lag between
    register writes and the hardware actually becoming busy.
    
    To fix this, the driver has been reworked to explicitly track the state
    of the hardware, rather than try to guess what it is doing based on the
    register values.
    
    This has passed dmatest with 10 threads per channel, 100000 iterations
    per thread several times without error. Previously, this would fail
    within a few seconds.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 5da1a4a817e3..6e9ad6edc4af 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -68,11 +68,6 @@ static dma_addr_t get_cdar(struct fsldma_chan *chan)
 	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;cdar, 64) &amp; ~FSL_DMA_SNEN;
 }
 
-static dma_addr_t get_ndar(struct fsldma_chan *chan)
-{
-	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;ndar, 64);
-}
-
 static u32 get_bcr(struct fsldma_chan *chan)
 {
 	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;bcr, 32);
@@ -143,13 +138,11 @@ static void dma_init(struct fsldma_chan *chan)
 	case FSL_DMA_IP_85XX:
 		/* Set the channel to below modes:
 		 * EIE - Error interrupt enable
-		 * EOSIE - End of segments interrupt enable (basic mode)
 		 * EOLNIE - End of links interrupt enable
 		 * BWC - Bandwidth sharing among channels
 		 */
 		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, FSL_DMA_MR_BWC
-				| FSL_DMA_MR_EIE | FSL_DMA_MR_EOLNIE
-				| FSL_DMA_MR_EOSIE, 32);
+				| FSL_DMA_MR_EIE | FSL_DMA_MR_EOLNIE, 32);
 		break;
 	case FSL_DMA_IP_83XX:
 		/* Set the channel to below modes:
@@ -168,25 +161,32 @@ static int dma_is_idle(struct fsldma_chan *chan)
 	return (!(sr &amp; FSL_DMA_SR_CB)) || (sr &amp; FSL_DMA_SR_CH);
 }
 
+/*
+ * Start the DMA controller
+ *
+ * Preconditions:
+ * - the CDAR register must point to the start descriptor
+ * - the MRn[CS] bit must be cleared
+ */
 static void dma_start(struct fsldma_chan *chan)
 {
 	u32 mode;
 
 	mode = DMA_IN(chan, &amp;chan-&gt;regs-&gt;mr, 32);
 
-	if ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {
-		if (chan-&gt;feature &amp; FSL_DMA_CHAN_PAUSE_EXT) {
-			DMA_OUT(chan, &amp;chan-&gt;regs-&gt;bcr, 0, 32);
-			mode |= FSL_DMA_MR_EMP_EN;
-		} else {
-			mode &amp;= ~FSL_DMA_MR_EMP_EN;
-		}
+	if (chan-&gt;feature &amp; FSL_DMA_CHAN_PAUSE_EXT) {
+		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;bcr, 0, 32);
+		mode |= FSL_DMA_MR_EMP_EN;
+	} else {
+		mode &amp;= ~FSL_DMA_MR_EMP_EN;
 	}
 
-	if (chan-&gt;feature &amp; FSL_DMA_CHAN_START_EXT)
+	if (chan-&gt;feature &amp; FSL_DMA_CHAN_START_EXT) {
 		mode |= FSL_DMA_MR_EMS_EN;
-	else
+	} else {
+		mode &amp;= ~FSL_DMA_MR_EMS_EN;
 		mode |= FSL_DMA_MR_CS;
+	}
 
 	DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, mode, 32);
 }
@@ -760,14 +760,15 @@ static int fsl_dma_device_control(struct dma_chan *dchan,
 
 	switch (cmd) {
 	case DMA_TERMINATE_ALL:
+		spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
+
 		/* Halt the DMA engine */
 		dma_halt(chan);
 
-		spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
-
 		/* Remove and free all of the descriptors in the LD queue */
 		fsldma_free_desc_list(chan, &amp;chan-&gt;ld_pending);
 		fsldma_free_desc_list(chan, &amp;chan-&gt;ld_running);
+		chan-&gt;idle = true;
 
 		spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
 		return 0;
@@ -805,76 +806,43 @@ static int fsl_dma_device_control(struct dma_chan *dchan,
 }
 
 /**
- * fsl_dma_update_completed_cookie - Update the completed cookie.
+ * fsl_chan_ld_cleanup - Clean up link descriptors
  * @chan : Freescale DMA channel
  *
- * CONTEXT: hardirq
+ * This function is run after the queue of running descriptors has been
+ * executed by the DMA engine. It will run any callbacks, and then free
+ * the descriptors.
+ *
+ * HARDWARE STATE: idle
  */
-static void fsl_dma_update_completed_cookie(struct fsldma_chan *chan)
+static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 {
-	struct fsl_desc_sw *desc;
+	struct fsl_desc_sw *desc, *_desc;
 	unsigned long flags;
-	dma_cookie_t cookie;
 
 	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
+	/* if the ld_running list is empty, there is nothing to do */
 	if (list_empty(&amp;chan-&gt;ld_running)) {
-		chan_dbg(chan, "no running descriptors\n");
+		chan_dbg(chan, "no descriptors to cleanup\n");
 		goto out_unlock;
 	}
 
-	/* Get the last descriptor, update the cookie to that */
+	/*
+	 * Get the last descriptor, update the cookie to it
+	 *
+	 * This is done before callbacks run so that clients can check the
+	 * status of their DMA transfer inside the callback.
+	 */
 	desc = to_fsl_desc(chan-&gt;ld_running.prev);
-	if (dma_is_idle(chan))
-		cookie = desc-&gt;async_tx.cookie;
-	else {
-		cookie = desc-&gt;async_tx.cookie - 1;
-		if (unlikely(cookie &lt; DMA_MIN_COOKIE))
-			cookie = DMA_MAX_COOKIE;
-	}
-
-	chan-&gt;completed_cookie = cookie;
-
-out_unlock:
-	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
-}
-
-/**
- * fsldma_desc_status - Check the status of a descriptor
- * @chan: Freescale DMA channel
- * @desc: DMA SW descriptor
- *
- * This function will return the status of the given descriptor
- */
-static enum dma_status fsldma_desc_status(struct fsldma_chan *chan,
-					  struct fsl_desc_sw *desc)
-{
-	return dma_async_is_complete(desc-&gt;async_tx.cookie,
-				     chan-&gt;completed_cookie,
-				     chan-&gt;common.cookie);
-}
-
-/**
- * fsl_chan_ld_cleanup - Clean up link descriptors
- * @chan : Freescale DMA channel
- *
- * This function clean up the ld_queue of DMA channel.
- */
-static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
-{
-	struct fsl_desc_sw *desc, *_desc;
-	unsigned long flags;
-
-	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
+	chan-&gt;completed_cookie = desc-&gt;async_tx.cookie;
+	chan_dbg(chan, "completed_cookie = %d\n", chan-&gt;completed_cookie);
 
-	chan_dbg(chan, "chan completed_cookie = %d\n", chan-&gt;completed_cookie);
+	/* Run the callback for each descriptor, in order */
 	list_for_each_entry_safe(desc, _desc, &amp;chan-&gt;ld_running, node) {
 		dma_async_tx_callback callback;
 		void *callback_param;
 
-		if (fsldma_desc_status(chan, desc) == DMA_IN_PROGRESS)
-			break;
-
 		/* Remove from the list of running transactions */
 		list_del(&amp;desc-&gt;node);
 
@@ -898,6 +866,7 @@ static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 		dma_pool_free(chan-&gt;desc_pool, desc, desc-&gt;async_tx.phys);
 	}
 
+out_unlock:
 	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
 }
 
@@ -905,10 +874,7 @@ static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
  * fsl_chan_xfer_ld_queue - transfer any pending transactions
  * @chan : Freescale DMA channel
  *
- * This will make sure that any pending transactions will be run.
- * If the DMA controller is idle, it will be started. Otherwise,
- * the DMA controller's interrupt handler will start any pending
- * transactions when it becomes idle.
+ * HARDWARE STATE: idle
  */
 static void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)
 {
@@ -927,22 +893,15 @@ static void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)
 	}
 
 	/*
-	 * The DMA controller is not idle, which means the interrupt
-	 * handler will start any queued transactions when it runs
-	 * at the end of the current transaction
+	 * The DMA controller is not idle, which means that the interrupt
+	 * handler will start any queued transactions when it runs after
+	 * this transaction finishes
 	 */
-	if (!dma_is_idle(chan)) {
+	if (!chan-&gt;idle) {
 		chan_dbg(chan, "DMA controller still busy\n");
 		goto out_unlock;
 	}
 
-	/*
-	 * TODO:
-	 * make sure the dma_halt() function really un-wedges the
-	 * controller as much as possible
-	 */
-	dma_halt(chan);
-
 	/*
 	 * If there are some link descriptors which have not been
 	 * transferred, we need to start the controller
@@ -952,15 +911,32 @@ static void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)
 	 * Move all elements from the queue of pending transactions
 	 * onto the list of running transactions
 	 */
+	chan_dbg(chan, "idle, starting controller\n");
 	desc = list_first_entry(&amp;chan-&gt;ld_pending, struct fsl_desc_sw, node);
 	list_splice_tail_init(&amp;chan-&gt;ld_pending, &amp;chan-&gt;ld_running);
 
+	/*
+	 * The 85xx DMA controller doesn't clear the channel start bit
+	 * automatically at the end of a transfer. Therefore we must clear
+	 * it in software before starting the transfer.
+	 */
+	if ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {
+		u32 mode;
+
+		mode = DMA_IN(chan, &amp;chan-&gt;regs-&gt;mr, 32);
+		mode &amp;= ~FSL_DMA_MR_CS;
+		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, mode, 32);
+	}
+
 	/*
 	 * Program the descriptor's address into the DMA controller,
 	 * then start the DMA transaction
 	 */
 	set_cdar(chan, desc-&gt;async_tx.phys);
+	get_cdar(chan);
+
 	dma_start(chan);
+	chan-&gt;idle = false;
 
 out_unlock:
 	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
@@ -985,16 +961,18 @@ static enum dma_status fsl_tx_status(struct dma_chan *dchan,
 					struct dma_tx_state *txstate)
 {
 	struct fsldma_chan *chan = to_fsl_chan(dchan);
-	dma_cookie_t last_used;
 	dma_cookie_t last_complete;
+	dma_cookie_t last_used;
+	unsigned long flags;
 
-	fsl_chan_ld_cleanup(chan);
+	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
-	last_used = dchan-&gt;cookie;
 	last_complete = chan-&gt;completed_cookie;
+	last_used = dchan-&gt;cookie;
 
-	dma_set_tx_state(txstate, last_complete, last_used, 0);
+	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
 
+	dma_set_tx_state(txstate, last_complete, last_used, 0);
 	return dma_async_is_complete(cookie, last_complete, last_used);
 }
 
@@ -1005,8 +983,6 @@ static enum dma_status fsl_tx_status(struct dma_chan *dchan,
 static irqreturn_t fsldma_chan_irq(int irq, void *data)
 {
 	struct fsldma_chan *chan = data;
-	int update_cookie = 0;
-	int xfer_ld_q = 0;
 	u32 stat;
 
 	/* save and clear the status register */
@@ -1014,6 +990,7 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	set_sr(chan, stat);
 	chan_dbg(chan, "irq: stat = 0x%x\n", stat);
 
+	/* check that this was really our device */
 	stat &amp;= ~(FSL_DMA_SR_CB | FSL_DMA_SR_CH);
 	if (!stat)
 		return IRQ_NONE;
@@ -1028,28 +1005,9 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	 */
 	if (stat &amp; FSL_DMA_SR_PE) {
 		chan_dbg(chan, "irq: Programming Error INT\n");
-		if (get_bcr(chan) == 0) {
-			/* BCR register is 0, this is a DMA_INTERRUPT async_tx.
-			 * Now, update the completed cookie, and continue the
-			 * next uncompleted transfer.
-			 */
-			update_cookie = 1;
-			xfer_ld_q = 1;
-		}
 		stat &amp;= ~FSL_DMA_SR_PE;
-	}
-
-	/*
-	 * If the link descriptor segment transfer finishes,
-	 * we will recycle the used descriptor.
-	 */
-	if (stat &amp; FSL_DMA_SR_EOSI) {
-		chan_dbg(chan, "irq: End-of-segments INT\n");
-		chan_dbg(chan, "irq: clndar 0x%llx, nlndar 0x%llx\n",
-			(unsigned long long)get_cdar(chan),
-			(unsigned long long)get_ndar(chan));
-		stat &amp;= ~FSL_DMA_SR_EOSI;
-		update_cookie = 1;
+		if (get_bcr(chan) != 0)
+			chan_err(chan, "Programming Error!\n");
 	}
 
 	/*
@@ -1059,8 +1017,6 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	if (stat &amp; FSL_DMA_SR_EOCDI) {
 		chan_dbg(chan, "irq: End-of-Chain link INT\n");
 		stat &amp;= ~FSL_DMA_SR_EOCDI;
-		update_cookie = 1;
-		xfer_ld_q = 1;
 	}
 
 	/*
@@ -1071,25 +1027,44 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	if (stat &amp; FSL_DMA_SR_EOLNI) {
 		chan_dbg(chan, "irq: End-of-link INT\n");
 		stat &amp;= ~FSL_DMA_SR_EOLNI;
-		xfer_ld_q = 1;
 	}
 
-	if (update_cookie)
-		fsl_dma_update_completed_cookie(chan);
-	if (xfer_ld_q)
-		fsl_chan_xfer_ld_queue(chan);
+	/* check that the DMA controller is really idle */
+	if (!dma_is_idle(chan))
+		chan_err(chan, "irq: controller not idle!\n");
+
+	/* check that we handled all of the bits */
 	if (stat)
-		chan_dbg(chan, "irq: unhandled sr 0x%08x\n", stat);
+		chan_err(chan, "irq: unhandled sr 0x%08x\n", stat);
 
-	chan_dbg(chan, "irq: Exit\n");
+	/*
+	 * Schedule the tasklet to handle all cleanup of the current
+	 * transaction. It will start a new transaction if there is
+	 * one pending.
+	 */
 	tasklet_schedule(&amp;chan-&gt;tasklet);
+	chan_dbg(chan, "irq: Exit\n");
 	return IRQ_HANDLED;
 }
 
 static void dma_do_tasklet(unsigned long data)
 {
 	struct fsldma_chan *chan = (struct fsldma_chan *)data;
+	unsigned long flags;
+
+	chan_dbg(chan, "tasklet entry\n");
+
+	/* run all callbacks, free all used descriptors */
 	fsl_chan_ld_cleanup(chan);
+
+	/* the channel is now idle */
+	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
+	chan-&gt;idle = true;
+	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
+
+	/* start any pending transactions automatically */
+	fsl_chan_xfer_ld_queue(chan);
+	chan_dbg(chan, "tasklet exit\n");
 }
 
 static irqreturn_t fsldma_ctrl_irq(int irq, void *data)
@@ -1269,6 +1244,7 @@ static int __devinit fsl_dma_chan_probe(struct fsldma_device *fdev,
 	spin_lock_init(&amp;chan-&gt;desc_lock);
 	INIT_LIST_HEAD(&amp;chan-&gt;ld_pending);
 	INIT_LIST_HEAD(&amp;chan-&gt;ld_running);
+	chan-&gt;idle = true;
 
 	chan-&gt;common.device = &amp;fdev-&gt;common;
 
diff --git a/drivers/dma/fsldma.h b/drivers/dma/fsldma.h
index 49189dacd5f4..9cb5aa57c677 100644
--- a/drivers/dma/fsldma.h
+++ b/drivers/dma/fsldma.h
@@ -148,6 +148,7 @@ struct fsldma_chan {
 	int id;				/* Raw id of this channel */
 	struct tasklet_struct tasklet;
 	u32 feature;
+	bool idle;			/* DMA controller is idle */
 
 	void (*toggle_ext_pause)(struct fsldma_chan *fsl_chan, int enable);
 	void (*toggle_ext_start)(struct fsldma_chan *fsl_chan, int enable);</pre><hr><pre>commit 31f4306c83a2daa3e348056b720de511bffe5a9b
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:57 2011 +0000

    fsldma: minor codingstyle and consistency fixes
    
    This fixes some minor violations of the coding style. It also changes
    the style of the device_prep_dma_*() function definitions so they are
    identical.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 82b8e9f9c7bf..5da1a4a817e3 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -89,7 +89,7 @@ static void set_desc_cnt(struct fsldma_chan *chan,
 }
 
 static void set_desc_src(struct fsldma_chan *chan,
-				struct fsl_dma_ld_hw *hw, dma_addr_t src)
+			 struct fsl_dma_ld_hw *hw, dma_addr_t src)
 {
 	u64 snoop_bits;
 
@@ -99,7 +99,7 @@ static void set_desc_src(struct fsldma_chan *chan,
 }
 
 static void set_desc_dst(struct fsldma_chan *chan,
-				struct fsl_dma_ld_hw *hw, dma_addr_t dst)
+			 struct fsl_dma_ld_hw *hw, dma_addr_t dst)
 {
 	u64 snoop_bits;
 
@@ -109,7 +109,7 @@ static void set_desc_dst(struct fsldma_chan *chan,
 }
 
 static void set_desc_next(struct fsldma_chan *chan,
-				struct fsl_dma_ld_hw *hw, dma_addr_t next)
+			  struct fsl_dma_ld_hw *hw, dma_addr_t next)
 {
 	u64 snoop_bits;
 
@@ -118,8 +118,7 @@ static void set_desc_next(struct fsldma_chan *chan,
 	hw-&gt;next_ln_addr = CPU_TO_DMA(chan, snoop_bits | next, 64);
 }
 
-static void set_ld_eol(struct fsldma_chan *chan,
-			struct fsl_desc_sw *desc)
+static void set_ld_eol(struct fsldma_chan *chan, struct fsl_desc_sw *desc)
 {
 	u64 snoop_bits;
 
@@ -338,8 +337,7 @@ static void fsl_chan_toggle_ext_start(struct fsldma_chan *chan, int enable)
 		chan-&gt;feature &amp;= ~FSL_DMA_CHAN_START_EXT;
 }
 
-static void append_ld_queue(struct fsldma_chan *chan,
-			    struct fsl_desc_sw *desc)
+static void append_ld_queue(struct fsldma_chan *chan, struct fsl_desc_sw *desc)
 {
 	struct fsl_desc_sw *tail = to_fsl_desc(chan-&gt;ld_pending.prev);
 
@@ -380,8 +378,8 @@ static dma_cookie_t fsl_dma_tx_submit(struct dma_async_tx_descriptor *tx)
 	cookie = chan-&gt;common.cookie;
 	list_for_each_entry(child, &amp;desc-&gt;tx_list, node) {
 		cookie++;
-		if (cookie &lt; 0)
-			cookie = 1;
+		if (cookie &lt; DMA_MIN_COOKIE)
+			cookie = DMA_MIN_COOKIE;
 
 		child-&gt;async_tx.cookie = cookie;
 	}
@@ -402,8 +400,7 @@ static dma_cookie_t fsl_dma_tx_submit(struct dma_async_tx_descriptor *tx)
  *
  * Return - The descriptor allocated. NULL for failed.
  */
-static struct fsl_desc_sw *fsl_dma_alloc_descriptor(
-					struct fsldma_chan *chan)
+static struct fsl_desc_sw *fsl_dma_alloc_descriptor(struct fsldma_chan *chan)
 {
 	struct fsl_desc_sw *desc;
 	dma_addr_t pdesc;
@@ -427,7 +424,6 @@ static struct fsl_desc_sw *fsl_dma_alloc_descriptor(
 	return desc;
 }
 
-
 /**
  * fsl_dma_alloc_chan_resources - Allocate resources for DMA channel.
  * @chan : Freescale DMA channel
@@ -537,14 +533,15 @@ fsl_dma_prep_interrupt(struct dma_chan *dchan, unsigned long flags)
 	/* Insert the link descriptor to the LD ring */
 	list_add_tail(&amp;new-&gt;node, &amp;new-&gt;tx_list);
 
-	/* Set End-of-link to the last link descriptor of new list*/
+	/* Set End-of-link to the last link descriptor of new list */
 	set_ld_eol(chan, new);
 
 	return &amp;new-&gt;async_tx;
 }
 
-static struct dma_async_tx_descriptor *fsl_dma_prep_memcpy(
-	struct dma_chan *dchan, dma_addr_t dma_dst, dma_addr_t dma_src,
+static struct dma_async_tx_descriptor *
+fsl_dma_prep_memcpy(struct dma_chan *dchan,
+	dma_addr_t dma_dst, dma_addr_t dma_src,
 	size_t len, unsigned long flags)
 {
 	struct fsldma_chan *chan;
@@ -594,7 +591,7 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_memcpy(
 	new-&gt;async_tx.flags = flags; /* client is in control of this ack */
 	new-&gt;async_tx.cookie = -EBUSY;
 
-	/* Set End-of-link to the last link descriptor of new list*/
+	/* Set End-of-link to the last link descriptor of new list */
 	set_ld_eol(chan, new);
 
 	return &amp;first-&gt;async_tx;
diff --git a/drivers/dma/fsldma.h b/drivers/dma/fsldma.h
index 113e7134010b..49189dacd5f4 100644
--- a/drivers/dma/fsldma.h
+++ b/drivers/dma/fsldma.h
@@ -102,8 +102,8 @@ struct fsl_desc_sw {
 } __attribute__((aligned(32)));
 
 struct fsldma_chan_regs {
-	u32 mr;	/* 0x00 - Mode Register */
-	u32 sr;	/* 0x04 - Status Register */
+	u32 mr;		/* 0x00 - Mode Register */
+	u32 sr;		/* 0x04 - Status Register */
 	u64 cdar;	/* 0x08 - Current descriptor address register */
 	u64 sar;	/* 0x10 - Source Address Register */
 	u64 dar;	/* 0x18 - Destination Address Register */</pre><hr><pre>commit 0ab09c36818ca88f65c88f4d8c6d067fbf10578d
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:56 2011 +0000

    fsldma: improve link descriptor debugging
    
    This adds better tracking to link descriptor allocations, callbacks, and
    frees. This makes it much easier to track errors with link descriptors.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index e535cd13f7cc..82b8e9f9c7bf 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -420,6 +420,10 @@ static struct fsl_desc_sw *fsl_dma_alloc_descriptor(
 	desc-&gt;async_tx.tx_submit = fsl_dma_tx_submit;
 	desc-&gt;async_tx.phys = pdesc;
 
+#ifdef FSL_DMA_LD_DEBUG
+	chan_dbg(chan, "LD %p allocated\n", desc);
+#endif
+
 	return desc;
 }
 
@@ -470,6 +474,9 @@ static void fsldma_free_desc_list(struct fsldma_chan *chan,
 
 	list_for_each_entry_safe(desc, _desc, list, node) {
 		list_del(&amp;desc-&gt;node);
+#ifdef FSL_DMA_LD_DEBUG
+		chan_dbg(chan, "LD %p free\n", desc);
+#endif
 		dma_pool_free(chan-&gt;desc_pool, desc, desc-&gt;async_tx.phys);
 	}
 }
@@ -481,6 +488,9 @@ static void fsldma_free_desc_list_reverse(struct fsldma_chan *chan,
 
 	list_for_each_entry_safe_reverse(desc, _desc, list, node) {
 		list_del(&amp;desc-&gt;node);
+#ifdef FSL_DMA_LD_DEBUG
+		chan_dbg(chan, "LD %p free\n", desc);
+#endif
 		dma_pool_free(chan-&gt;desc_pool, desc, desc-&gt;async_tx.phys);
 	}
 }
@@ -557,9 +567,6 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_memcpy(
 			chan_err(chan, "%s\n", msg_ld_oom);
 			goto fail;
 		}
-#ifdef FSL_DMA_LD_DEBUG
-		chan_dbg(chan, "new link desc alloc %p\n", new);
-#endif
 
 		copy = min(len, (size_t)FSL_DMA_BCR_MAX_CNT);
 
@@ -645,9 +652,6 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_sg(struct dma_chan *dchan,
 			chan_err(chan, "%s\n", msg_ld_oom);
 			goto fail;
 		}
-#ifdef FSL_DMA_LD_DEBUG
-		chan_dbg(chan, "new link desc alloc %p\n", new);
-#endif
 
 		set_desc_cnt(chan, &amp;new-&gt;hw, len);
 		set_desc_src(chan, &amp;new-&gt;hw, src);
@@ -882,13 +886,18 @@ static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 		callback_param = desc-&gt;async_tx.callback_param;
 		if (callback) {
 			spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
+#ifdef FSL_DMA_LD_DEBUG
 			chan_dbg(chan, "LD %p callback\n", desc);
+#endif
 			callback(callback_param);
 			spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 		}
 
 		/* Run any dependencies, then free the descriptor */
 		dma_run_dependencies(&amp;desc-&gt;async_tx);
+#ifdef FSL_DMA_LD_DEBUG
+		chan_dbg(chan, "LD %p free\n", desc);
+#endif
 		dma_pool_free(chan-&gt;desc_pool, desc, desc-&gt;async_tx.phys);
 	}
 </pre><hr><pre>commit b158471ef63bf399165db96e945a828096502d9d
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:55 2011 +0000

    fsldma: use channel name in printk output
    
    This makes debugging the driver much easier when multiple channels are
    running concurrently. In addition, you can see how much descriptor
    memory each channel has allocated via the dmapool API in sysfs.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 2e1af4555b0f..e535cd13f7cc 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -37,7 +37,12 @@
 
 #include "fsldma.h"
 
-static const char msg_ld_oom[] = "No free memory for link descriptor\n";
+#define chan_dbg(chan, fmt, arg...)					\
+	dev_dbg(chan-&gt;dev, "%s: " fmt, chan-&gt;name, ##arg)
+#define chan_err(chan, fmt, arg...)					\
+	dev_err(chan-&gt;dev, "%s: " fmt, chan-&gt;name, ##arg)
+
+static const char msg_ld_oom[] = "No free memory for link descriptor";
 
 /*
  * Register Helpers
@@ -207,7 +212,7 @@ static void dma_halt(struct fsldma_chan *chan)
 	}
 
 	if (!dma_is_idle(chan))
-		dev_err(chan-&gt;dev, "DMA halt timeout!\n");
+		chan_err(chan, "DMA halt timeout!\n");
 }
 
 /**
@@ -405,7 +410,7 @@ static struct fsl_desc_sw *fsl_dma_alloc_descriptor(
 
 	desc = dma_pool_alloc(chan-&gt;desc_pool, GFP_ATOMIC, &amp;pdesc);
 	if (!desc) {
-		dev_dbg(chan-&gt;dev, "out of memory for link desc\n");
+		chan_dbg(chan, "out of memory for link descriptor\n");
 		return NULL;
 	}
 
@@ -439,13 +444,11 @@ static int fsl_dma_alloc_chan_resources(struct dma_chan *dchan)
 	 * We need the descriptor to be aligned to 32bytes
 	 * for meeting FSL DMA specification requirement.
 	 */
-	chan-&gt;desc_pool = dma_pool_create("fsl_dma_engine_desc_pool",
-					  chan-&gt;dev,
+	chan-&gt;desc_pool = dma_pool_create(chan-&gt;name, chan-&gt;dev,
 					  sizeof(struct fsl_desc_sw),
 					  __alignof__(struct fsl_desc_sw), 0);
 	if (!chan-&gt;desc_pool) {
-		dev_err(chan-&gt;dev, "unable to allocate channel %d "
-				   "descriptor pool\n", chan-&gt;id);
+		chan_err(chan, "unable to allocate descriptor pool\n");
 		return -ENOMEM;
 	}
 
@@ -491,7 +494,7 @@ static void fsl_dma_free_chan_resources(struct dma_chan *dchan)
 	struct fsldma_chan *chan = to_fsl_chan(dchan);
 	unsigned long flags;
 
-	dev_dbg(chan-&gt;dev, "Free all channel resources.\n");
+	chan_dbg(chan, "free all channel resources\n");
 	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 	fsldma_free_desc_list(chan, &amp;chan-&gt;ld_pending);
 	fsldma_free_desc_list(chan, &amp;chan-&gt;ld_running);
@@ -514,7 +517,7 @@ fsl_dma_prep_interrupt(struct dma_chan *dchan, unsigned long flags)
 
 	new = fsl_dma_alloc_descriptor(chan);
 	if (!new) {
-		dev_err(chan-&gt;dev, msg_ld_oom);
+		chan_err(chan, "%s\n", msg_ld_oom);
 		return NULL;
 	}
 
@@ -551,11 +554,11 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_memcpy(
 		/* Allocate the link descriptor from DMA pool */
 		new = fsl_dma_alloc_descriptor(chan);
 		if (!new) {
-			dev_err(chan-&gt;dev, msg_ld_oom);
+			chan_err(chan, "%s\n", msg_ld_oom);
 			goto fail;
 		}
 #ifdef FSL_DMA_LD_DEBUG
-		dev_dbg(chan-&gt;dev, "new link desc alloc %p\n", new);
+		chan_dbg(chan, "new link desc alloc %p\n", new);
 #endif
 
 		copy = min(len, (size_t)FSL_DMA_BCR_MAX_CNT);
@@ -639,11 +642,11 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_sg(struct dma_chan *dchan,
 		/* allocate and populate the descriptor */
 		new = fsl_dma_alloc_descriptor(chan);
 		if (!new) {
-			dev_err(chan-&gt;dev, msg_ld_oom);
+			chan_err(chan, "%s\n", msg_ld_oom);
 			goto fail;
 		}
 #ifdef FSL_DMA_LD_DEBUG
-		dev_dbg(chan-&gt;dev, "new link desc alloc %p\n", new);
+		chan_dbg(chan, "new link desc alloc %p\n", new);
 #endif
 
 		set_desc_cnt(chan, &amp;new-&gt;hw, len);
@@ -815,7 +818,7 @@ static void fsl_dma_update_completed_cookie(struct fsldma_chan *chan)
 	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
 	if (list_empty(&amp;chan-&gt;ld_running)) {
-		dev_dbg(chan-&gt;dev, "no running descriptors\n");
+		chan_dbg(chan, "no running descriptors\n");
 		goto out_unlock;
 	}
 
@@ -863,7 +866,7 @@ static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 
 	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
-	dev_dbg(chan-&gt;dev, "chan completed_cookie = %d\n", chan-&gt;completed_cookie);
+	chan_dbg(chan, "chan completed_cookie = %d\n", chan-&gt;completed_cookie);
 	list_for_each_entry_safe(desc, _desc, &amp;chan-&gt;ld_running, node) {
 		dma_async_tx_callback callback;
 		void *callback_param;
@@ -879,7 +882,7 @@ static void fsl_chan_ld_cleanup(struct fsldma_chan *chan)
 		callback_param = desc-&gt;async_tx.callback_param;
 		if (callback) {
 			spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
-			dev_dbg(chan-&gt;dev, "LD %p callback\n", desc);
+			chan_dbg(chan, "LD %p callback\n", desc);
 			callback(callback_param);
 			spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 		}
@@ -913,7 +916,7 @@ static void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)
 	 * don't need to do any work at all
 	 */
 	if (list_empty(&amp;chan-&gt;ld_pending)) {
-		dev_dbg(chan-&gt;dev, "no pending LDs\n");
+		chan_dbg(chan, "no pending LDs\n");
 		goto out_unlock;
 	}
 
@@ -923,7 +926,7 @@ static void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)
 	 * at the end of the current transaction
 	 */
 	if (!dma_is_idle(chan)) {
-		dev_dbg(chan-&gt;dev, "DMA controller still busy\n");
+		chan_dbg(chan, "DMA controller still busy\n");
 		goto out_unlock;
 	}
 
@@ -1003,14 +1006,14 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	/* save and clear the status register */
 	stat = get_sr(chan);
 	set_sr(chan, stat);
-	dev_dbg(chan-&gt;dev, "irq: channel %d, stat = 0x%x\n", chan-&gt;id, stat);
+	chan_dbg(chan, "irq: stat = 0x%x\n", stat);
 
 	stat &amp;= ~(FSL_DMA_SR_CB | FSL_DMA_SR_CH);
 	if (!stat)
 		return IRQ_NONE;
 
 	if (stat &amp; FSL_DMA_SR_TE)
-		dev_err(chan-&gt;dev, "Transfer Error!\n");
+		chan_err(chan, "Transfer Error!\n");
 
 	/*
 	 * Programming Error
@@ -1018,7 +1021,7 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	 * triger a PE interrupt.
 	 */
 	if (stat &amp; FSL_DMA_SR_PE) {
-		dev_dbg(chan-&gt;dev, "irq: Programming Error INT\n");
+		chan_dbg(chan, "irq: Programming Error INT\n");
 		if (get_bcr(chan) == 0) {
 			/* BCR register is 0, this is a DMA_INTERRUPT async_tx.
 			 * Now, update the completed cookie, and continue the
@@ -1035,8 +1038,8 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	 * we will recycle the used descriptor.
 	 */
 	if (stat &amp; FSL_DMA_SR_EOSI) {
-		dev_dbg(chan-&gt;dev, "irq: End-of-segments INT\n");
-		dev_dbg(chan-&gt;dev, "irq: clndar 0x%llx, nlndar 0x%llx\n",
+		chan_dbg(chan, "irq: End-of-segments INT\n");
+		chan_dbg(chan, "irq: clndar 0x%llx, nlndar 0x%llx\n",
 			(unsigned long long)get_cdar(chan),
 			(unsigned long long)get_ndar(chan));
 		stat &amp;= ~FSL_DMA_SR_EOSI;
@@ -1048,7 +1051,7 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	 * and start the next transfer if it exist.
 	 */
 	if (stat &amp; FSL_DMA_SR_EOCDI) {
-		dev_dbg(chan-&gt;dev, "irq: End-of-Chain link INT\n");
+		chan_dbg(chan, "irq: End-of-Chain link INT\n");
 		stat &amp;= ~FSL_DMA_SR_EOCDI;
 		update_cookie = 1;
 		xfer_ld_q = 1;
@@ -1060,7 +1063,7 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	 * prepare next transfer.
 	 */
 	if (stat &amp; FSL_DMA_SR_EOLNI) {
-		dev_dbg(chan-&gt;dev, "irq: End-of-link INT\n");
+		chan_dbg(chan, "irq: End-of-link INT\n");
 		stat &amp;= ~FSL_DMA_SR_EOLNI;
 		xfer_ld_q = 1;
 	}
@@ -1070,9 +1073,9 @@ static irqreturn_t fsldma_chan_irq(int irq, void *data)
 	if (xfer_ld_q)
 		fsl_chan_xfer_ld_queue(chan);
 	if (stat)
-		dev_dbg(chan-&gt;dev, "irq: unhandled sr 0x%02x\n", stat);
+		chan_dbg(chan, "irq: unhandled sr 0x%08x\n", stat);
 
-	dev_dbg(chan-&gt;dev, "irq: Exit\n");
+	chan_dbg(chan, "irq: Exit\n");
 	tasklet_schedule(&amp;chan-&gt;tasklet);
 	return IRQ_HANDLED;
 }
@@ -1128,7 +1131,7 @@ static void fsldma_free_irqs(struct fsldma_device *fdev)
 	for (i = 0; i &lt; FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {
 		chan = fdev-&gt;chan[i];
 		if (chan &amp;&amp; chan-&gt;irq != NO_IRQ) {
-			dev_dbg(fdev-&gt;dev, "free channel %d IRQ\n", chan-&gt;id);
+			chan_dbg(chan, "free per-channel IRQ\n");
 			free_irq(chan-&gt;irq, chan);
 		}
 	}
@@ -1155,19 +1158,16 @@ static int fsldma_request_irqs(struct fsldma_device *fdev)
 			continue;
 
 		if (chan-&gt;irq == NO_IRQ) {
-			dev_err(fdev-&gt;dev, "no interrupts property defined for "
-					   "DMA channel %d. Please fix your "
-					   "device tree\n", chan-&gt;id);
+			chan_err(chan, "interrupts property missing in device tree\n");
 			ret = -ENODEV;
 			goto out_unwind;
 		}
 
-		dev_dbg(fdev-&gt;dev, "request channel %d IRQ\n", chan-&gt;id);
+		chan_dbg(chan, "request per-channel IRQ\n");
 		ret = request_irq(chan-&gt;irq, fsldma_chan_irq, IRQF_SHARED,
 				  "fsldma-chan", chan);
 		if (ret) {
-			dev_err(fdev-&gt;dev, "unable to request IRQ for DMA "
-					   "channel %d\n", chan-&gt;id);
+			chan_err(chan, "unable to request per-channel IRQ\n");
 			goto out_unwind;
 		}
 	}
@@ -1242,6 +1242,7 @@ static int __devinit fsl_dma_chan_probe(struct fsldma_device *fdev,
 
 	fdev-&gt;chan[chan-&gt;id] = chan;
 	tasklet_init(&amp;chan-&gt;tasklet, dma_do_tasklet, (unsigned long)chan);
+	snprintf(chan-&gt;name, sizeof(chan-&gt;name), "chan%d", chan-&gt;id);
 
 	/* Initialize the channel */
 	dma_init(chan);
diff --git a/drivers/dma/fsldma.h b/drivers/dma/fsldma.h
index ba9f403c0fbe..113e7134010b 100644
--- a/drivers/dma/fsldma.h
+++ b/drivers/dma/fsldma.h
@@ -135,6 +135,7 @@ struct fsldma_device {
 #define FSL_DMA_CHAN_START_EXT	0x00002000
 
 struct fsldma_chan {
+	char name[8];			/* Channel name */
 	struct fsldma_chan_regs __iomem *regs;
 	dma_cookie_t completed_cookie;	/* The maximum cookie completed */
 	spinlock_t desc_lock;		/* Descriptor operation lock */</pre><hr><pre>commit e8bd84df27c5921a9ac866aef06e044590ac118f
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:54 2011 +0000

    fsldma: move related helper functions near each other
    
    This is a purely cosmetic cleanup. It is nice to have related functions
    right next to each other in the code.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 4de947a450fc..2e1af4555b0f 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -39,33 +39,9 @@
 
 static const char msg_ld_oom[] = "No free memory for link descriptor\n";
 
-static void dma_init(struct fsldma_chan *chan)
-{
-	/* Reset the channel */
-	DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, 0, 32);
-
-	switch (chan-&gt;feature &amp; FSL_DMA_IP_MASK) {
-	case FSL_DMA_IP_85XX:
-		/* Set the channel to below modes:
-		 * EIE - Error interrupt enable
-		 * EOSIE - End of segments interrupt enable (basic mode)
-		 * EOLNIE - End of links interrupt enable
-		 * BWC - Bandwidth sharing among channels
-		 */
-		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, FSL_DMA_MR_BWC
-				| FSL_DMA_MR_EIE | FSL_DMA_MR_EOLNIE
-				| FSL_DMA_MR_EOSIE, 32);
-		break;
-	case FSL_DMA_IP_83XX:
-		/* Set the channel to below modes:
-		 * EOTIE - End-of-transfer interrupt enable
-		 * PRC_RM - PCI read multiple
-		 */
-		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, FSL_DMA_MR_EOTIE
-				| FSL_DMA_MR_PRC_RM, 32);
-		break;
-	}
-}
+/*
+ * Register Helpers
+ */
 
 static void set_sr(struct fsldma_chan *chan, u32 val)
 {
@@ -77,6 +53,30 @@ static u32 get_sr(struct fsldma_chan *chan)
 	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;sr, 32);
 }
 
+static void set_cdar(struct fsldma_chan *chan, dma_addr_t addr)
+{
+	DMA_OUT(chan, &amp;chan-&gt;regs-&gt;cdar, addr | FSL_DMA_SNEN, 64);
+}
+
+static dma_addr_t get_cdar(struct fsldma_chan *chan)
+{
+	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;cdar, 64) &amp; ~FSL_DMA_SNEN;
+}
+
+static dma_addr_t get_ndar(struct fsldma_chan *chan)
+{
+	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;ndar, 64);
+}
+
+static u32 get_bcr(struct fsldma_chan *chan)
+{
+	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;bcr, 32);
+}
+
+/*
+ * Descriptor Helpers
+ */
+
 static void set_desc_cnt(struct fsldma_chan *chan,
 				struct fsl_dma_ld_hw *hw, u32 count)
 {
@@ -113,24 +113,49 @@ static void set_desc_next(struct fsldma_chan *chan,
 	hw-&gt;next_ln_addr = CPU_TO_DMA(chan, snoop_bits | next, 64);
 }
 
-static void set_cdar(struct fsldma_chan *chan, dma_addr_t addr)
+static void set_ld_eol(struct fsldma_chan *chan,
+			struct fsl_desc_sw *desc)
 {
-	DMA_OUT(chan, &amp;chan-&gt;regs-&gt;cdar, addr | FSL_DMA_SNEN, 64);
-}
+	u64 snoop_bits;
 
-static dma_addr_t get_cdar(struct fsldma_chan *chan)
-{
-	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;cdar, 64) &amp; ~FSL_DMA_SNEN;
-}
+	snoop_bits = ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)
+		? FSL_DMA_SNEN : 0;
 
-static dma_addr_t get_ndar(struct fsldma_chan *chan)
-{
-	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;ndar, 64);
+	desc-&gt;hw.next_ln_addr = CPU_TO_DMA(chan,
+		DMA_TO_CPU(chan, desc-&gt;hw.next_ln_addr, 64) | FSL_DMA_EOL
+			| snoop_bits, 64);
 }
 
-static u32 get_bcr(struct fsldma_chan *chan)
+/*
+ * DMA Engine Hardware Control Helpers
+ */
+
+static void dma_init(struct fsldma_chan *chan)
 {
-	return DMA_IN(chan, &amp;chan-&gt;regs-&gt;bcr, 32);
+	/* Reset the channel */
+	DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, 0, 32);
+
+	switch (chan-&gt;feature &amp; FSL_DMA_IP_MASK) {
+	case FSL_DMA_IP_85XX:
+		/* Set the channel to below modes:
+		 * EIE - Error interrupt enable
+		 * EOSIE - End of segments interrupt enable (basic mode)
+		 * EOLNIE - End of links interrupt enable
+		 * BWC - Bandwidth sharing among channels
+		 */
+		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, FSL_DMA_MR_BWC
+				| FSL_DMA_MR_EIE | FSL_DMA_MR_EOLNIE
+				| FSL_DMA_MR_EOSIE, 32);
+		break;
+	case FSL_DMA_IP_83XX:
+		/* Set the channel to below modes:
+		 * EOTIE - End-of-transfer interrupt enable
+		 * PRC_RM - PCI read multiple
+		 */
+		DMA_OUT(chan, &amp;chan-&gt;regs-&gt;mr, FSL_DMA_MR_EOTIE
+				| FSL_DMA_MR_PRC_RM, 32);
+		break;
+	}
 }
 
 static int dma_is_idle(struct fsldma_chan *chan)
@@ -185,19 +210,6 @@ static void dma_halt(struct fsldma_chan *chan)
 		dev_err(chan-&gt;dev, "DMA halt timeout!\n");
 }
 
-static void set_ld_eol(struct fsldma_chan *chan,
-			struct fsl_desc_sw *desc)
-{
-	u64 snoop_bits;
-
-	snoop_bits = ((chan-&gt;feature &amp; FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)
-		? FSL_DMA_SNEN : 0;
-
-	desc-&gt;hw.next_ln_addr = CPU_TO_DMA(chan,
-		DMA_TO_CPU(chan, desc-&gt;hw.next_ln_addr, 64) | FSL_DMA_EOL
-			| snoop_bits, 64);
-}
-
 /**
  * fsl_chan_set_src_loop_size - Set source address hold transfer size
  * @chan : Freescale DMA channel</pre><hr><pre>commit b203bd3f6b9c3db3b1979c2ff79bb2b9be8f03a3
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Mar 3 07:54:53 2011 +0000

    dmatest: fix automatic buffer unmap type
    
    The dmatest code relies on the DMAEngine API to automatically call
    dma_unmap_single() on src buffers. The flags it passes are incorrect,
    fix them.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/dmatest.c b/drivers/dma/dmatest.c
index 5589358b684d..7e1b0aa0ca50 100644
--- a/drivers/dma/dmatest.c
+++ b/drivers/dma/dmatest.c
@@ -285,7 +285,12 @@ static int dmatest_func(void *data)
 
 	set_user_nice(current, 10);
 
-	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_DEST_UNMAP | DMA_PREP_INTERRUPT;
+	/*
+	 * src buffers are freed by the DMAEngine code with dma_unmap_single()
+	 * dst buffers are freed by ourselves below
+	 */
+	flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT
+	      | DMA_COMPL_SKIP_DEST_UNMAP | DMA_COMPL_SRC_UNMAP_SINGLE;
 
 	while (!kthread_should_stop()
 	       &amp;&amp; !(iterations &amp;&amp; total_tests &gt;= iterations)) {</pre><hr><pre>commit 0a6bf658c3b9d1d0e520d320b6392c8680c2e381
Author: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Mon Dec 13 11:42:30 2010 -0500

    hwmon: (ltc4215) make sysfs file match the alarm cause
    
    The ltc4215 driver used the chip's "power good" status bit to provide
    the power1_alarm file. This is wrong: the chip is really reporting the
    status of one of the monitored voltages.
    
    Change the sysfs file from power1_alarm to in2_min_alarm instead. This
    matches the voltage that the chip is raising an alarm for.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Guenter Roeck &lt;guenter.roeck@ericsson.com&gt;

diff --git a/drivers/hwmon/ltc4215.c b/drivers/hwmon/ltc4215.c
index 00d975eb5b83..c7e6d8e81656 100644
--- a/drivers/hwmon/ltc4215.c
+++ b/drivers/hwmon/ltc4215.c
@@ -205,7 +205,6 @@ LTC4215_ALARM(curr1_max_alarm,	(1 &lt;&lt; 2),	LTC4215_STATUS);
 
 /* Power (virtual) */
 LTC4215_POWER(power1_input);
-LTC4215_ALARM(power1_alarm,	(1 &lt;&lt; 3),	LTC4215_STATUS);
 
 /* Input Voltage */
 LTC4215_VOLTAGE(in1_input,			LTC4215_ADIN);
@@ -214,6 +213,7 @@ LTC4215_ALARM(in1_min_alarm,	(1 &lt;&lt; 1),	LTC4215_STATUS);
 
 /* Output Voltage */
 LTC4215_VOLTAGE(in2_input,			LTC4215_SOURCE);
+LTC4215_ALARM(in2_min_alarm,	(1 &lt;&lt; 3),	LTC4215_STATUS);
 
 /* Finally, construct an array of pointers to members of the above objects,
  * as required for sysfs_create_group()
@@ -223,13 +223,13 @@ static struct attribute *ltc4215_attributes[] = {
 	&amp;sensor_dev_attr_curr1_max_alarm.dev_attr.attr,
 
 	&amp;sensor_dev_attr_power1_input.dev_attr.attr,
-	&amp;sensor_dev_attr_power1_alarm.dev_attr.attr,
 
 	&amp;sensor_dev_attr_in1_input.dev_attr.attr,
 	&amp;sensor_dev_attr_in1_max_alarm.dev_attr.attr,
 	&amp;sensor_dev_attr_in1_min_alarm.dev_attr.attr,
 
 	&amp;sensor_dev_attr_in2_input.dev_attr.attr,
+	&amp;sensor_dev_attr_in2_min_alarm.dev_attr.attr,
 
 	NULL,
 };</pre><hr><pre>commit 0d688662aab9d80078be82aa5aea561346643298
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Sep 30 11:46:47 2010 +0000

    ste_dma40: implement support for scatterlist to scatterlist copy
    
    Now that the DMAEngine API has support for scatterlist to scatterlist
    copy, implement support for the STE DMA40 DMA controller.
    
    Cc: Linus Walleij &lt;linus.ml.walleij@gmail.com&gt;
    Acked-by: Per Frid√©n &lt;per.friden@stericsson.com&gt;
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/drivers/dma/ste_dma40.c b/drivers/dma/ste_dma40.c
index 17e2600a00cf..d5fd098e22e8 100644
--- a/drivers/dma/ste_dma40.c
+++ b/drivers/dma/ste_dma40.c
@@ -1857,6 +1857,18 @@ static struct dma_async_tx_descriptor *d40_prep_memcpy(struct dma_chan *chan,
 	return NULL;
 }
 
+static struct dma_async_tx_descriptor *
+d40_prep_sg(struct dma_chan *chan,
+	    struct scatterlist *dst_sg, unsigned int dst_nents,
+	    struct scatterlist *src_sg, unsigned int src_nents,
+	    unsigned long dma_flags)
+{
+	if (dst_nents != src_nents)
+		return NULL;
+
+	return stedma40_memcpy_sg(chan, dst_sg, src_sg, dst_nents, dma_flags);
+}
+
 static int d40_prep_slave_sg_log(struct d40_desc *d40d,
 				 struct d40_chan *d40c,
 				 struct scatterlist *sgl,
@@ -2281,6 +2293,7 @@ static int __init d40_dmaengine_init(struct d40_base *base,
 	base-&gt;dma_slave.device_alloc_chan_resources = d40_alloc_chan_resources;
 	base-&gt;dma_slave.device_free_chan_resources = d40_free_chan_resources;
 	base-&gt;dma_slave.device_prep_dma_memcpy = d40_prep_memcpy;
+	base-&gt;dma_slave.device_prep_dma_sg = d40_prep_sg;
 	base-&gt;dma_slave.device_prep_slave_sg = d40_prep_slave_sg;
 	base-&gt;dma_slave.device_tx_status = d40_tx_status;
 	base-&gt;dma_slave.device_issue_pending = d40_issue_pending;
@@ -2301,10 +2314,12 @@ static int __init d40_dmaengine_init(struct d40_base *base,
 
 	dma_cap_zero(base-&gt;dma_memcpy.cap_mask);
 	dma_cap_set(DMA_MEMCPY, base-&gt;dma_memcpy.cap_mask);
+	dma_cap_set(DMA_SG, base-&gt;dma_slave.cap_mask);
 
 	base-&gt;dma_memcpy.device_alloc_chan_resources = d40_alloc_chan_resources;
 	base-&gt;dma_memcpy.device_free_chan_resources = d40_free_chan_resources;
 	base-&gt;dma_memcpy.device_prep_dma_memcpy = d40_prep_memcpy;
+	base-&gt;dma_slave.device_prep_dma_sg = d40_prep_sg;
 	base-&gt;dma_memcpy.device_prep_slave_sg = d40_prep_slave_sg;
 	base-&gt;dma_memcpy.device_tx_status = d40_tx_status;
 	base-&gt;dma_memcpy.device_issue_pending = d40_issue_pending;
@@ -2331,10 +2346,12 @@ static int __init d40_dmaengine_init(struct d40_base *base,
 	dma_cap_zero(base-&gt;dma_both.cap_mask);
 	dma_cap_set(DMA_SLAVE, base-&gt;dma_both.cap_mask);
 	dma_cap_set(DMA_MEMCPY, base-&gt;dma_both.cap_mask);
+	dma_cap_set(DMA_SG, base-&gt;dma_slave.cap_mask);
 
 	base-&gt;dma_both.device_alloc_chan_resources = d40_alloc_chan_resources;
 	base-&gt;dma_both.device_free_chan_resources = d40_free_chan_resources;
 	base-&gt;dma_both.device_prep_dma_memcpy = d40_prep_memcpy;
+	base-&gt;dma_slave.device_prep_dma_sg = d40_prep_sg;
 	base-&gt;dma_both.device_prep_slave_sg = d40_prep_slave_sg;
 	base-&gt;dma_both.device_tx_status = d40_tx_status;
 	base-&gt;dma_both.device_issue_pending = d40_issue_pending;</pre><hr><pre>commit 968f19ae802fdc6b6b6b5af6fe79cf23d281be0f
Author: Ira Snyder &lt;iws@ovro.caltech.edu&gt;
Date:   Thu Sep 30 11:46:46 2010 +0000

    fsldma: improved DMA_SLAVE support
    
    Now that the generic DMAEngine API has support for scatterlist to
    scatterlist copying, the device_prep_slave_sg() portion of the
    DMA_SLAVE API is no longer necessary and has been removed.
    
    However, the device_control() portion of the DMA_SLAVE API is still
    useful to control device specific parameters, such as externally
    controlled DMA transfers and maximum burst length.
    
    A special dma_ctrl_cmd has been added to enable externally controlled
    DMA transfers. This is currently specific to the Freescale DMA
    controller, but can easily be made generic when another user is found.
    
    Signed-off-by: Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
    Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;

diff --git a/arch/powerpc/include/asm/fsldma.h b/arch/powerpc/include/asm/fsldma.h
deleted file mode 100644
index debc5ed96d6e..000000000000
--- a/arch/powerpc/include/asm/fsldma.h
+++ /dev/null
@@ -1,137 +0,0 @@
-/*
- * Freescale MPC83XX / MPC85XX DMA Controller
- *
- * Copyright (c) 2009 Ira W. Snyder &lt;iws@ovro.caltech.edu&gt;
- *
- * This file is licensed under the terms of the GNU General Public License
- * version 2. This program is licensed "as is" without any warranty of any
- * kind, whether express or implied.
- */
-
-#ifndef __ARCH_POWERPC_ASM_FSLDMA_H__
-#define __ARCH_POWERPC_ASM_FSLDMA_H__
-
-#include &lt;linux/slab.h&gt;
-#include &lt;linux/dmaengine.h&gt;
-
-/*
- * Definitions for the Freescale DMA controller's DMA_SLAVE implemention
- *
- * The Freescale DMA_SLAVE implementation was designed to handle many-to-many
- * transfers. An example usage would be an accelerated copy between two
- * scatterlists. Another example use would be an accelerated copy from
- * multiple non-contiguous device buffers into a single scatterlist.
- *
- * A DMA_SLAVE transaction is defined by a struct fsl_dma_slave. This
- * structure contains a list of hardware addresses that should be copied
- * to/from the scatterlist passed into device_prep_slave_sg(). The structure
- * also has some fields to enable hardware-specific features.
- */
-
-/**
- * struct fsl_dma_hw_addr
- * @entry: linked list entry
- * @address: the hardware address
- * @length: length to transfer
- *
- * Holds a single physical hardware address / length pair for use
- * with the DMAEngine DMA_SLAVE API.
- */
-struct fsl_dma_hw_addr {
-	struct list_head entry;
-
-	dma_addr_t address;
-	size_t length;
-};
-
-/**
- * struct fsl_dma_slave
- * @addresses: a linked list of struct fsl_dma_hw_addr structures
- * @request_count: value for DMA request count
- * @src_loop_size: setup and enable constant source-address DMA transfers
- * @dst_loop_size: setup and enable constant destination address DMA transfers
- * @external_start: enable externally started DMA transfers
- * @external_pause: enable externally paused DMA transfers
- *
- * Holds a list of address / length pairs for use with the DMAEngine
- * DMA_SLAVE API implementation for the Freescale DMA controller.
- */
-struct fsl_dma_slave {
-
-	/* List of hardware address/length pairs */
-	struct list_head addresses;
-
-	/* Support for extra controller features */
-	unsigned int request_count;
-	unsigned int src_loop_size;
-	unsigned int dst_loop_size;
-	bool external_start;
-	bool external_pause;
-};
-
-/**
- * fsl_dma_slave_append - add an address/length pair to a struct fsl_dma_slave
- * @slave: the &amp;struct fsl_dma_slave to add to
- * @address: the hardware address to add
- * @length: the length of bytes to transfer from @address
- *
- * Add a hardware address/length pair to a struct fsl_dma_slave. Returns 0 on
- * success, -ERRNO otherwise.
- */
-static inline int fsl_dma_slave_append(struct fsl_dma_slave *slave,
-				       dma_addr_t address, size_t length)
-{
-	struct fsl_dma_hw_addr *addr;
-
-	addr = kzalloc(sizeof(*addr), GFP_ATOMIC);
-	if (!addr)
-		return -ENOMEM;
-
-	INIT_LIST_HEAD(&amp;addr-&gt;entry);
-	addr-&gt;address = address;
-	addr-&gt;length = length;
-
-	list_add_tail(&amp;addr-&gt;entry, &amp;slave-&gt;addresses);
-	return 0;
-}
-
-/**
- * fsl_dma_slave_free - free a struct fsl_dma_slave
- * @slave: the struct fsl_dma_slave to free
- *
- * Free a struct fsl_dma_slave and all associated address/length pairs
- */
-static inline void fsl_dma_slave_free(struct fsl_dma_slave *slave)
-{
-	struct fsl_dma_hw_addr *addr, *tmp;
-
-	if (slave) {
-		list_for_each_entry_safe(addr, tmp, &amp;slave-&gt;addresses, entry) {
-			list_del(&amp;addr-&gt;entry);
-			kfree(addr);
-		}
-
-		kfree(slave);
-	}
-}
-
-/**
- * fsl_dma_slave_alloc - allocate a struct fsl_dma_slave
- * @gfp: the flags to pass to kmalloc when allocating this structure
- *
- * Allocate a struct fsl_dma_slave for use by the DMA_SLAVE API. Returns a new
- * struct fsl_dma_slave on success, or NULL on failure.
- */
-static inline struct fsl_dma_slave *fsl_dma_slave_alloc(gfp_t gfp)
-{
-	struct fsl_dma_slave *slave;
-
-	slave = kzalloc(sizeof(*slave), gfp);
-	if (!slave)
-		return NULL;
-
-	INIT_LIST_HEAD(&amp;slave-&gt;addresses);
-	return slave;
-}
-
-#endif /* __ARCH_POWERPC_ASM_FSLDMA_H__ */
diff --git a/drivers/dma/fsldma.c b/drivers/dma/fsldma.c
index 1ed29d10a5fa..286c3ac6bdcc 100644
--- a/drivers/dma/fsldma.c
+++ b/drivers/dma/fsldma.c
@@ -35,7 +35,6 @@
 #include &lt;linux/dmapool.h&gt;
 #include &lt;linux/of_platform.h&gt;
 
-#include &lt;asm/fsldma.h&gt;
 #include "fsldma.h"
 
 static const char msg_ld_oom[] = "No free memory for link descriptor\n";
@@ -719,207 +718,70 @@ static struct dma_async_tx_descriptor *fsl_dma_prep_slave_sg(
 	struct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,
 	enum dma_data_direction direction, unsigned long flags)
 {
-	struct fsldma_chan *chan;
-	struct fsl_desc_sw *first = NULL, *prev = NULL, *new = NULL;
-	struct fsl_dma_slave *slave;
-	size_t copy;
-
-	int i;
-	struct scatterlist *sg;
-	size_t sg_used;
-	size_t hw_used;
-	struct fsl_dma_hw_addr *hw;
-	dma_addr_t dma_dst, dma_src;
-
-	if (!dchan)
-		return NULL;
-
-	if (!dchan-&gt;private)
-		return NULL;
-
-	chan = to_fsl_chan(dchan);
-	slave = dchan-&gt;private;
-
-	if (list_empty(&amp;slave-&gt;addresses))
-		return NULL;
-
-	hw = list_first_entry(&amp;slave-&gt;addresses, struct fsl_dma_hw_addr, entry);
-	hw_used = 0;
-
 	/*
-	 * Build the hardware transaction to copy from the scatterlist to
-	 * the hardware, or from the hardware to the scatterlist
+	 * This operation is not supported on the Freescale DMA controller
 	 *
-	 * If you are copying from the hardware to the scatterlist and it
-	 * takes two hardware entries to fill an entire page, then both
-	 * hardware entries will be coalesced into the same page
-	 *
-	 * If you are copying from the scatterlist to the hardware and a
-	 * single page can fill two hardware entries, then the data will
-	 * be read out of the page into the first hardware entry, and so on
+	 * However, we need to provide the function pointer to allow the
+	 * device_control() method to work.
 	 */
-	for_each_sg(sgl, sg, sg_len, i) {
-		sg_used = 0;
-
-		/* Loop until the entire scatterlist entry is used */
-		while (sg_used &lt; sg_dma_len(sg)) {
-
-			/*
-			 * If we've used up the current hardware address/length
-			 * pair, we need to load a new one
-			 *
-			 * This is done in a while loop so that descriptors with
-			 * length == 0 will be skipped
-			 */
-			while (hw_used &gt;= hw-&gt;length) {
-
-				/*
-				 * If the current hardware entry is the last
-				 * entry in the list, we're finished
-				 */
-				if (list_is_last(&amp;hw-&gt;entry, &amp;slave-&gt;addresses))
-					goto finished;
-
-				/* Get the next hardware address/length pair */
-				hw = list_entry(hw-&gt;entry.next,
-						struct fsl_dma_hw_addr, entry);
-				hw_used = 0;
-			}
-
-			/* Allocate the link descriptor from DMA pool */
-			new = fsl_dma_alloc_descriptor(chan);
-			if (!new) {
-				dev_err(chan-&gt;dev, "No free memory for "
-						       "link descriptor\n");
-				goto fail;
-			}
-#ifdef FSL_DMA_LD_DEBUG
-			dev_dbg(chan-&gt;dev, "new link desc alloc %p\n", new);
-#endif
-
-			/*
-			 * Calculate the maximum number of bytes to transfer,
-			 * making sure it is less than the DMA controller limit
-			 */
-			copy = min_t(size_t, sg_dma_len(sg) - sg_used,
-					     hw-&gt;length - hw_used);
-			copy = min_t(size_t, copy, FSL_DMA_BCR_MAX_CNT);
-
-			/*
-			 * DMA_FROM_DEVICE
-			 * from the hardware to the scatterlist
-			 *
-			 * DMA_TO_DEVICE
-			 * from the scatterlist to the hardware
-			 */
-			if (direction == DMA_FROM_DEVICE) {
-				dma_src = hw-&gt;address + hw_used;
-				dma_dst = sg_dma_address(sg) + sg_used;
-			} else {
-				dma_src = sg_dma_address(sg) + sg_used;
-				dma_dst = hw-&gt;address + hw_used;
-			}
-
-			/* Fill in the descriptor */
-			set_desc_cnt(chan, &amp;new-&gt;hw, copy);
-			set_desc_src(chan, &amp;new-&gt;hw, dma_src);
-			set_desc_dst(chan, &amp;new-&gt;hw, dma_dst);
-
-			/*
-			 * If this is not the first descriptor, chain the
-			 * current descriptor after the previous descriptor
-			 */
-			if (!first) {
-				first = new;
-			} else {
-				set_desc_next(chan, &amp;prev-&gt;hw,
-					      new-&gt;async_tx.phys);
-			}
-
-			new-&gt;async_tx.cookie = 0;
-			async_tx_ack(&amp;new-&gt;async_tx);
-
-			prev = new;
-			sg_used += copy;
-			hw_used += copy;
-
-			/* Insert the link descriptor into the LD ring */
-			list_add_tail(&amp;new-&gt;node, &amp;first-&gt;tx_list);
-		}
-	}
-
-finished:
-
-	/* All of the hardware address/length pairs had length == 0 */
-	if (!first || !new)
-		return NULL;
-
-	new-&gt;async_tx.flags = flags;
-	new-&gt;async_tx.cookie = -EBUSY;
-
-	/* Set End-of-link to the last link descriptor of new list */
-	set_ld_eol(chan, new);
-
-	/* Enable extra controller features */
-	if (chan-&gt;set_src_loop_size)
-		chan-&gt;set_src_loop_size(chan, slave-&gt;src_loop_size);
-
-	if (chan-&gt;set_dst_loop_size)
-		chan-&gt;set_dst_loop_size(chan, slave-&gt;dst_loop_size);
-
-	if (chan-&gt;toggle_ext_start)
-		chan-&gt;toggle_ext_start(chan, slave-&gt;external_start);
-
-	if (chan-&gt;toggle_ext_pause)
-		chan-&gt;toggle_ext_pause(chan, slave-&gt;external_pause);
-
-	if (chan-&gt;set_request_count)
-		chan-&gt;set_request_count(chan, slave-&gt;request_count);
-
-	return &amp;first-&gt;async_tx;
-
-fail:
-	/* If first was not set, then we failed to allocate the very first
-	 * descriptor, and we're done */
-	if (!first)
-		return NULL;
-
-	/*
-	 * First is set, so all of the descriptors we allocated have been added
-	 * to first-&gt;tx_list, INCLUDING "first" itself. Therefore we
-	 * must traverse the list backwards freeing each descriptor in turn
-	 *
-	 * We're re-using variables for the loop, oh well
-	 */
-	fsldma_free_desc_list_reverse(chan, &amp;first-&gt;tx_list);
 	return NULL;
 }
 
 static int fsl_dma_device_control(struct dma_chan *dchan,
 				  enum dma_ctrl_cmd cmd, unsigned long arg)
 {
+	struct dma_slave_config *config;
 	struct fsldma_chan *chan;
 	unsigned long flags;
-
-	/* Only supports DMA_TERMINATE_ALL */
-	if (cmd != DMA_TERMINATE_ALL)
-		return -ENXIO;
+	int size;
 
 	if (!dchan)
 		return -EINVAL;
 
 	chan = to_fsl_chan(dchan);
 
-	/* Halt the DMA engine */
-	dma_halt(chan);
+	switch (cmd) {
+	case DMA_TERMINATE_ALL:
+		/* Halt the DMA engine */
+		dma_halt(chan);
 
-	spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
+		spin_lock_irqsave(&amp;chan-&gt;desc_lock, flags);
 
-	/* Remove and free all of the descriptors in the LD queue */
-	fsldma_free_desc_list(chan, &amp;chan-&gt;ld_pending);
-	fsldma_free_desc_list(chan, &amp;chan-&gt;ld_running);
+		/* Remove and free all of the descriptors in the LD queue */
+		fsldma_free_desc_list(chan, &amp;chan-&gt;ld_pending);
+		fsldma_free_desc_list(chan, &amp;chan-&gt;ld_running);
 
-	spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
+		spin_unlock_irqrestore(&amp;chan-&gt;desc_lock, flags);
+		return 0;
+
+	case DMA_SLAVE_CONFIG:
+		config = (struct dma_slave_config *)arg;
+
+		/* make sure the channel supports setting burst size */
+		if (!chan-&gt;set_request_count)
+			return -ENXIO;
+
+		/* we set the controller burst size depending on direction */
+		if (config-&gt;direction == DMA_TO_DEVICE)
+			size = config-&gt;dst_addr_width * config-&gt;dst_maxburst;
+		else
+			size = config-&gt;src_addr_width * config-&gt;src_maxburst;
+
+		chan-&gt;set_request_count(chan, size);
+		return 0;
+
+	case FSLDMA_EXTERNAL_START:
+
+		/* make sure the channel supports external start */
+		if (!chan-&gt;toggle_ext_start)
+			return -ENXIO;
+
+		chan-&gt;toggle_ext_start(chan, arg);
+		return 0;
+
+	default:
+		return -ENXIO;
+	}
 
 	return 0;
 }
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 2c9ee98f6c77..885f35211675 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -120,12 +120,15 @@ enum dma_ctrl_flags {
  * configuration data in statically from the platform). An additional
  * argument of struct dma_slave_config must be passed in with this
  * command.
+ * @FSLDMA_EXTERNAL_START: this command will put the Freescale DMA controller
+ * into external start mode.
  */
 enum dma_ctrl_cmd {
 	DMA_TERMINATE_ALL,
 	DMA_PAUSE,
 	DMA_RESUME,
 	DMA_SLAVE_CONFIG,
+	FSLDMA_EXTERNAL_START,
 };
 
 /**</pre>
    <div class="pagination">
        <a href='16_2.html'>&lt;&lt;Prev</a><a href='16.html'>1</a><a href='16_2.html'>2</a><span>[3]</span><a href='16_4.html'>4</a><a href='16_5.html'>5</a><a href='16_6.html'>6</a><a href='16_7.html'>7</a><a href='16_4.html'>Next&gt;&gt;</a>
    <div>
</body>
